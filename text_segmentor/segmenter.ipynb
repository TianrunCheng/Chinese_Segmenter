{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmenter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "smfS39V_wpKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def preprocess(original_file, input_file, label_file):\n",
        "  \"\"\"\n",
        "  This function reads original training file\n",
        "  return: input file with no spaces and label file of Boolean sequences\n",
        "  \"\"\"\n",
        "\n",
        "  fi = open(input_file, 'w')\n",
        "  fl = open(label_file, 'w')\n",
        "\n",
        "  with open(original_file, 'r', encoding='utf8') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "      line_no_space = line.replace(' ', '')\n",
        "      fi.write(line_no_space)\n",
        "      line_of_bool_label = ''\n",
        "      for i in range(len(line)-1):\n",
        "        if line[i] is not ' ':\n",
        "          if line[i+1] is ' ' or line[i+1] is '\\n':\n",
        "            line_of_bool_label += '1'\n",
        "          else:\n",
        "            line_of_bool_label += '0'\n",
        "        else:\n",
        "          pass\n",
        "      line_of_bool_label += '\\n'\n",
        "      fl.write(line_of_bool_label)\n",
        "      line = f.readline()\n",
        "  fi.close()\n",
        "  fl.close()\n",
        "\n",
        "preprocess('msr_test_gold.utf8', 'test_input', 'test_label')\n",
        "preprocess('msr_training.utf8', 'train_input', 'train_label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv6hwaigk6oX",
        "colab_type": "code",
        "outputId": "70cef0be-1fdf-46b7-c8a6-64a056c4da4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from random import shuffle\n",
        "import numpy as np\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(torch.version.__version__, use_cuda)\n",
        "###############################################################################\n",
        "def cuda(arr):\n",
        "    if use_cuda:\n",
        "        return arr.cuda()\n",
        "    return arr\n",
        "\n",
        "###############################################################################\n",
        "# model paras\n",
        "###############################################################################\n",
        "char_embed_size = 30\n",
        "\n",
        "rnn_size = 300\n",
        "# dimension of hidden layer\n",
        "rnn_nLayers = 2\n",
        "\n",
        "RNN_layers = [rnn_size, rnn_nLayers]\n",
        "\n",
        "# feed-forward layers\n",
        "layer0 = 128\n",
        "layer1 = 64\n",
        "layer2 = 128\n",
        "FFNN_layers = [layer0, layer1, layer2]\n",
        "\n",
        "dropout = 0.1\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, specs):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    nChars, embed_size, rnn_layers, ffnn_layers, dropout = specs\n",
        "    self.CharEmbed = nn.Embedding(nChars, embed_size)\n",
        "\n",
        "    rnn_size, rnn_nLayers = rnn_layers\n",
        "    # self.rnn = nn.GRU(embed_size, rnn_size, rnn_nLayers, dropout=dropout, batch_first = True)\n",
        "    self.rnn = nn.LSTM(embed_size, rnn_size, rnn_nLayers, bidirectional=True, dropout=dropout, batch_first = True)\n",
        "\n",
        "    self.layers = nn.ModuleList([])\n",
        "    prev_size = rnn_size * 2\n",
        "    for i, layer_size in enumerate(ffnn_layers):\n",
        "        layer = nn.Linear(prev_size, layer_size)\n",
        "        self.layers.append(layer)\n",
        "        prev_size = layer_size\n",
        "\n",
        "    self.out = nn.Linear(prev_size, 2) # map to 2 classes\n",
        "\n",
        "    self.non_linear = nn.LeakyReLU(negative_slope=0.01)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    for p in self.parameters(): # optionally apply different randomization\n",
        "        if p.dim() > 1:\n",
        "            nn.init.kaiming_normal_(p)\n",
        "            pass\n",
        "\n",
        "  def forward(self, seqs, hidden=None):\n",
        "    # if seqs is a list of tensors already stored in cuda:\n",
        "    # nBatch = len(seqs)\n",
        "    # nChars = len(seqs[0])\n",
        "\n",
        "    # seqs = torch.cat(seqs).view(nBatch, nChars)\n",
        "\n",
        "    # if seqs is already tensor of shape[nBatch, nChars] on cuda, start here:\n",
        "    embed = self.CharEmbed(seqs)\n",
        "\n",
        "    prev, hidden = self.rnn(embed, hidden)\n",
        "\n",
        "    for layer in self.layers:\n",
        "        prev = layer(prev)\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "\n",
        "    out = self.out(prev) # chars\n",
        "    # print(out.size())\n",
        "    #hidden = torch.transpose(hidden, 0, 1)\n",
        "    return out, hidden\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzwLyO__1edY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# these values are used within the training code\n",
        "###############################################################################\n",
        "global learning_rate\n",
        "global batch_size\n",
        "global chunk_size\n",
        "global nEpochs\n",
        "global L2_lambda\n",
        "\n",
        "learning_rate = 0.0001\n",
        "learning_rate = 0.0003\n",
        "\n",
        "batch_size = 5\n",
        "batch_size = 100\n",
        "\n",
        "chunk_size = 50\n",
        "chunk_size = 100\n",
        "\n",
        "#nEpochs = 1\n",
        "nEpochs = 10\n",
        "\n",
        "L2_lambda = 0.0\n",
        "\n",
        "def RNN_train(model, optimizer, criterion, input_chunk, target_chunk, update=True):\n",
        "  model.zero_grad()\n",
        "  loss = 0\n",
        "  nFrames = 0\n",
        "\n",
        "  out, hidden = model(input_chunk)\n",
        "  \n",
        "  nBatch = len(input_chunk)\n",
        "  for i in range(nBatch):\n",
        "    loss += criterion(out[i][:,:], target_chunk[i][:])\n",
        "  \n",
        "  if update:\n",
        "    if not loss is 0:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  \n",
        "  return loss.data.item()\n",
        "\n",
        "\"\"\"\n",
        "the train_rnn_model function make batchs that can be directly feed into model: \n",
        "tensor of shape[nBatch, nChars]; nBatch = batch_size; nChars = seq_len\n",
        "Input: list of training sequences, Shape[N_lines, len(i_line)],\n",
        "       \\chars represented by int\n",
        "do: randomly choose some lines in the train set to make a batch\n",
        "making batch_in: pad all lines(with period?) to the batch max_len, convert chunk \n",
        "                \\into a tensor stored on cuda;\n",
        "making batch_tar: pad all lines(with 1?) to the batch max_len, convert chunk into\n",
        "                \\tensor on cuda\n",
        "\"\"\"\n",
        "def train_rnn_model(model, train_input, train_target):\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "\n",
        "  train_list  = [i for i in range(len(train_input))]    # list of indexes of training data\n",
        "\n",
        "  train_loss = 0\n",
        "  trained_chars = 0\n",
        "  input_pad = converter.word_to_int('。')\n",
        "  tar_pad = 1\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(nEpochs):\n",
        "    shuffle(train_list)\n",
        "    ith_batch = 0\n",
        "    for i in range(0, len(train_list), batch_size):\n",
        "      ith_batch += 1\n",
        "      batch_ndx = train_list[i:i+batch_size]\n",
        "      batch_in = []\n",
        "      batch_tar = []\n",
        "      l_max = 0 # max len sentence of this batch\n",
        "      for n in batch_ndx:\n",
        "        trained_chars += len(train_input[n])\n",
        "        l_max = max(l_max, len(train_input[n]))\n",
        "        batch_in.append(train_input[n])\n",
        "        batch_tar.append(train_target[n])\n",
        "      for n in range(len(batch_ndx)):\n",
        "        for x in range(l_max-len(batch_in[n])):\n",
        "          batch_in[n].append(input_pad)\n",
        "          batch_tar[n].append(tar_pad)\n",
        "      # now batch_in and batch_tar is shape[batch_size, l_max]\n",
        "      batch_in = torch.tensor(batch_in)\n",
        "      batch_in = cuda(batch_in)\n",
        "      batch_tar = torch.tensor(batch_tar)\n",
        "      batch_tar = cuda(batch_tar)\n",
        "\n",
        "      loss = RNN_train(model, optimizer, criterion, batch_in, batch_tar)\n",
        "\n",
        "      train_loss += loss\n",
        "      if ith_batch % 20 == 0:\n",
        "        print(\"%2d %6d %8.3f\" % (epoch, ith_batch*batch_size, train_loss/trained_chars))\n",
        "    torch.save(model, \"model.pth\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxoNhU5-EI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextConverter:\n",
        "  \"\"\"\n",
        "  build vocabulary(not include space, no <unk>) from all text(train and test)\n",
        "  create dictionary of {char:index}\n",
        "  and convert characters to index integers\n",
        "  \"\"\"\n",
        "  def __init__(self, text_path1, text_path2 = None):\n",
        "    vocab = {}\n",
        "    with open(text_path1, 'r') as f:\n",
        "      text = f.read()\n",
        "      if text_path2 is not None:\n",
        "        with open(text_path2, 'r') as f2:\n",
        "          text2 = f2.read()\n",
        "          text = text + text2\n",
        "    \n",
        "    ndx = 0\n",
        "    for c in text:\n",
        "      if c not in vocab and c is not ' ' and c is not '\\n':\n",
        "        vocab[c] = ndx\n",
        "        ndx += 1\n",
        "\n",
        "    self.vocab = vocab\n",
        "\n",
        "    self.vocab_size = len(vocab)\n",
        "\n",
        "    self.vocab_list = [0 for i in range(self.vocab_size)]\n",
        "    for c in vocab:\n",
        "      self.vocab_list[vocab[c]] = c\n",
        "  \n",
        "  def word_to_int(self, word):\n",
        "    return self.vocab[word]\n",
        "  \n",
        "  def int_to_word(self, index):\n",
        "    return self.vocab_list[index]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8IO9lgnASPZ",
        "colab_type": "code",
        "outputId": "8475e16c-1844-4e53-bb46-a54065e00c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#### main ####\n",
        "global converter\n",
        "converter = TextConverter('train_input', 'test_input')\n",
        "with open('train_input', 'r') as f:\n",
        "  corpus = f.readlines()\n",
        "with open('train_label', 'r') as f:\n",
        "  corpus_target = f.readlines()\n",
        "for i in range(len(corpus)):\n",
        "  corpus[i] = corpus[i].replace('\\n','')\n",
        "  # corpus = ['扬帆远东做与中国合作的先行', '希腊的经济结构较特殊。', ...]\n",
        "  corpus_target[i] = corpus_target[i].replace('\\n', '')\n",
        "  # corpus_target = ['0101110101101', '01101011011', ...]\n",
        "\n",
        "\"\"\"\n",
        "the main part of program then have two tasks:\n",
        "1. initialize neural network model for the train_rnn func\n",
        "1. generate train_input and train_output for the train_rnn func\n",
        "  \\ just convert the corpus to list of list of int(index of each char)\n",
        "  \\ convert the corpus_target to list of int(1) and int(0)\n",
        "\"\"\"\n",
        "\n",
        "train_input = []\n",
        "for line in corpus:\n",
        "  conv_line = []\n",
        "  for c in line:\n",
        "    conv_line.append(converter.word_to_int(c))\n",
        "  train_input.append(conv_line)\n",
        "print(train_input[:100])\n",
        "\n",
        "train_tar = []\n",
        "for line in corpus_target:\n",
        "  conv_line = []\n",
        "  for c in line:\n",
        "    conv_line.append(int(c))\n",
        "  train_tar.append(conv_line)\n",
        "print(train_tar[:100])\n",
        "\n",
        "nChars = len(converter.vocab_list)\n",
        "specs = [nChars, char_embed_size, RNN_layers, FFNN_layers, dropout]\n",
        "model = RNN(specs)\n",
        "print(model)\n",
        "\n",
        "train_rnn_model(model, train_input, train_tar)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 7, 22, 23, 24, 25, 18, 10, 11, 12, 13, 26, 27, 28, 7, 29, 30, 31, 28, 18, 32, 33, 18, 34, 35, 36, 37], [0, 38, 39, 40, 41, 42, 43, 44, 13, 45, 46, 47, 48, 49, 5, 50, 37], [0, 51, 52, 18, 53, 54, 13, 55, 8, 56, 57, 58, 59, 22, 60, 61, 62], [63, 0, 64, 65, 66, 67, 68, 69, 13, 70, 8, 71, 11, 72, 73, 74, 18, 75, 76, 77, 78, 8, 71, 79, 34, 18, 80, 81, 81, 82, 83, 13, 75, 14, 77, 78, 8, 84, 85, 73, 18, 86, 87, 88, 89, 81, 82, 19, 76, 13, 90, 74, 91, 92, 93, 18, 88, 89, 81, 82, 94, 95, 37], [0, 96, 14, 97, 98, 18, 99, 100, 101, 102, 103, 104, 18, 105, 100, 13, 22, 106, 107, 108, 37], [0, 109, 110, 111, 7, 71, 80, 112, 113, 114, 13, 80, 112, 18, 115, 116, 113, 114, 37], [36, 33, 117, 118, 119, 120, 121, 122, 123, 13, 63, 8, 124, 125, 73, 126, 124, 127, 37], [0, 128, 129, 111, 130, 131, 132, 18, 133, 102, 134, 135, 136, 13, 137, 22, 7, 88, 89, 81, 82, 133, 138, 139, 18, 140, 96, 37], [0, 109, 71, 141, 125, 70, 142, 143, 144, 33, 2, 145, 146, 102, 147, 148, 149, 150, 151, 152, 13, 58, 146, 102, 147, 153, 154, 155, 156, 154, 150, 151, 152, 37], [0, 157, 158, 102, 159, 94, 116, 160, 161, 13, 34, 162, 163, 164, 157, 37], [0, 165, 166, 125, 13, 167, 168, 169, 170, 171, 13, 23, 34, 172, 173, 116, 93, 174, 175, 13, 176, 177, 6, 145, 178, 174, 179, 13, 180, 145, 20, 174, 24, 181, 182, 90, 37], [0, 183, 61, 13, 184, 177, 58, 22, 185, 154, 186, 33, 8, 187, 62], [0, 188, 189, 13, 58, 102, 190, 191, 13, 165, 7, 192, 18, 193, 194, 195, 196, 87, 13, 34, 197, 189, 132, 198, 199, 37], [0, 188, 200, 66, 102, 71, 201, 202, 203, 13, 204, 205, 66, 206, 207, 208, 7, 209, 210, 18, 211, 212, 37], [0, 77, 213, 18, 214, 215, 13, 216, 217, 155, 218, 75, 155, 130, 219, 13, 145, 220, 221, 222, 152, 37], [0, 78, 176, 177, 34, 172, 132, 17, 223, 224, 176, 177, 225, 145, 226, 227, 13, 228, 118, 229, 84, 230, 22, 73, 187, 118, 175, 62], [169, 231, 8, 232, 224, 233, 234, 235, 236, 39, 237, 187, 13, 238, 0, 22, 66, 239, 240, 13, 241, 33, 2, 88, 242, 243, 19, 244, 245, 246, 247], [0, 132, 248, 249, 250, 18, 19, 251, 224, 13, 22, 131, 132, 252, 253, 254, 229, 18, 255, 84, 37], [0, 256, 193, 33, 257, 258, 38, 13, 259, 78, 109, 260, 125, 261, 262, 118, 37], [0, 33, 18, 263, 125, 264, 265, 266, 22, 267, 13, 238, 35, 268, 77, 269, 8, 270, 13, 271, 60, 181, 272, 24, 13, 273, 274, 22, 7, 140, 275, 276, 277, 13, 58, 22, 7, 140, 275, 278, 115, 37], [0, 279, 280, 281, 282, 283, 284, 284, 284, 1, 285, 286, 287, 155, 288, 287, 155, 289, 287, 18, 290, 291, 240, 37], [63, 135, 91, 0, 10, 292, 66, 293, 294, 248, 249, 250, 13, 293, 294, 295, 296, 13, 293, 294, 97, 185, 297, 195, 0, 33, 2, 66, 28, 248, 248, 249, 250, 13, 135, 298, 7, 11, 35, 252, 253, 66, 299, 224, 127, 37], [300, 301, 102, 273, 302, 303, 224, 185, 304, 305, 0, 34, 149, 297, 13, 306, 307, 308, 309, 228, 0, 310, 311, 247], [0, 312, 3, 313, 233, 314, 192, 314, 315, 13, 316, 317, 210, 155, 318, 319, 320, 58, 22, 321, 322, 1, 305, 323, 37], [0, 324, 130, 33, 325, 18, 326, 327, 1, 24, 328, 329, 132, 330, 331, 332, 155, 107, 333, 332, 155, 334, 105, 93, 153, 332, 155, 100, 335, 246, 93, 336, 332, 13, 337, 272, 328, 338, 37], [0, 339, 339, 13, 340, 248, 132, 341, 118, 13, 342, 60, 266, 335, 343, 344, 62], [0, 221, 222, 118, 109, 177, 24, 18, 345, 346, 347, 348, 13, 349, 308, 132, 74, 350, 18, 351, 71, 100, 335, 352, 124, 353, 354, 37], [0, 156, 27, 355, 78, 356, 357, 358, 359, 348, 13, 115, 48, 360, 130, 8, 361, 37], [0, 132, 172, 302, 337, 362, 13, 314, 102, 165, 177, 24, 363, 364, 365, 366, 13, 33, 367, 368, 369, 118, 13, 370, 125, 257, 371, 13, 5, 372, 373, 22, 209, 34, 172, 37], [0, 8, 71, 1, 66, 299, 160, 73, 8, 318, 94, 13, 374, 375, 160, 73, 8, 318, 376, 377, 13, 102, 133, 25, 13, 374, 102, 133, 239, 13, 109, 7, 378, 379, 18, 37], [36, 380, 381, 382, 235, 106, 383, 224, 384, 290, 385, 324, 13, 386, 387, 103, 8, 388, 124, 255, 389, 390, 37], [0, 132, 22, 391, 100, 335, 13, 32, 179, 24, 179, 391, 8, 71, 392, 36, 7, 49, 393, 179, 9, 390, 181, 394, 369, 395, 13, 49, 393, 179, 9, 396, 336, 75, 397, 18, 8, 71, 398, 66, 153, 399, 37], [0, 256, 130, 13, 33, 2, 18, 400, 401, 155, 336, 402, 155, 403, 404, 145, 293, 405, 174, 290, 18, 367, 406, 407, 408, 37], [0, 146, 102, 263, 125, 18, 295, 296, 7, 409, 410, 18, 13, 146, 102, 273, 1, 18, 295, 296, 7, 411, 412, 18, 37], [63, 4, 13, 109, 0, 22, 7, 8, 188, 413, 414, 13, 14, 7, 8, 188, 415, 416, 37], [0, 295, 296, 417, 418, 419, 60, 22, 7, 420, 180, 81, 421, 18, 113, 114, 13, 422, 423, 424, 7, 24, 425, 250, 18, 13, 14, 22, 7, 8, 425, 250, 18, 37], [0, 325, 426, 427, 428, 429, 430, 413, 431, 432, 433, 13, 434, 20, 435, 3, 218, 436, 37], [0, 33, 58, 437, 22, 73, 29, 438, 13, 146, 102, 1, 439, 440, 441, 188, 13, 422, 442, 22, 443, 100, 14, 5, 37], [0, 89, 444, 325, 155, 325, 426, 330, 444, 445, 446, 101, 330, 444, 447, 448, 325, 423, 424, 449, 450, 451, 452, 13, 132, 213, 453, 450, 207, 454, 455, 224, 456, 306, 290, 18, 444, 199, 350, 136, 37], [0, 33, 2, 22, 25, 22, 242, 38, 451, 457, 373, 458, 109, 459, 113, 114, 37], [0, 1, 2, 460, 174, 461, 240, 18, 38, 116, 228, 462, 342, 463, 37], [0, 70, 64, 13, 81, 66, 18, 155, 110, 66, 18, 7, 66, 70, 68, 69, 64, 209, 37], [0, 161, 22, 464, 465, 20, 185, 118, 32, 329, 466, 456, 467, 468, 36, 13, 230, 66, 469, 367, 470, 161, 68, 74, 456, 467, 37], [0, 471, 8, 158, 472, 473, 294, 474, 475, 160, 476, 289, 477, 304, 390, 181, 18, 335, 148, 478, 22, 479, 37], [0, 68, 420, 7, 8, 71, 22, 23, 328, 480, 18, 481, 350, 13, 8, 71, 1, 146, 102, 68, 420, 13, 34, 172, 230, 145, 146, 102, 68, 420, 37], [0, 109, 392, 482, 179, 9, 13, 8, 5, 299, 483, 224, 484, 485, 62], [0, 486, 215, 256, 487, 13, 215, 488, 489, 490, 13, 295, 296, 8, 491, 492, 493, 37], [63, 132, 494, 495, 496, 477, 304, 13, 28, 94, 67, 7, 13, 497, 498, 8, 499, 294, 130, 305, 500, 8, 501, 502, 13, 230, 503, 73, 118, 0, 504, 505, 506, 507, 13, 508, 509, 398, 510, 13, 121, 511, 512, 513, 13, 514, 515, 369, 516, 37], [0, 70, 64, 13, 81, 66, 18, 155, 110, 66, 18, 7, 66, 70, 68, 69, 64, 209, 37], [0, 161, 22, 464, 465, 20, 185, 118, 32, 329, 466, 456, 467, 468, 36, 13, 230, 66, 469, 367, 470, 161, 68, 74, 456, 467, 37], [0, 471, 8, 158, 472, 473, 294, 474, 475, 160, 476, 289, 477, 304, 390, 181, 18, 335, 148, 478, 22, 479, 37], [0, 68, 420, 7, 8, 71, 22, 23, 328, 480, 18, 481, 350, 13, 8, 71, 1, 146, 102, 68, 420, 13, 34, 172, 230, 145, 146, 102, 68, 420, 37], [0, 445, 517, 9, 440, 78, 118, 68, 342, 223, 517, 518, 13, 22, 241, 73, 519, 223, 132, 384, 232, 224, 520, 343, 521, 223, 13, 109, 7, 423, 424, 18, 37], [0, 33, 360, 121, 522, 81, 66, 7, 523, 121, 524, 525, 526, 527, 124, 13, 349, 18, 528, 404, 7, 161, 22, 161, 13, 161, 228, 176, 177, 529, 530, 13, 98, 18, 330, 531, 522, 532, 533, 534, 106, 535, 526, 161, 22, 161, 522, 23, 4, 380, 398, 66, 526, 13, 536, 134, 489, 537, 13, 475, 249, 489, 537, 13, 489, 538, 102, 539, 540, 13, 541, 22, 541, 13, 161, 22, 161, 37], [0, 248, 132, 145, 7, 138, 5, 125, 542, 13, 378, 193, 51, 25, 161, 155, 543, 25, 161, 13, 176, 177, 145, 22, 544, 13, 161, 24, 545, 546, 343, 547, 548, 263, 125, 2, 18, 290, 475, 549, 550, 551, 77, 118, 12, 37], [0, 238, 7, 305, 118, 552, 91, 12, 348, 18, 494, 381, 249, 338, 501, 319, 500, 324, 553, 347, 348, 13, 314, 554, 22, 555, 66, 4, 8, 556, 225, 37], [0, 216, 217, 501, 250, 13, 557, 558, 559, 213, 13, 560, 34, 561, 46, 13, 562, 563, 564, 565, 37], [0, 363, 566, 391, 567, 80, 112, 100, 332, 81, 66, 7, 305, 165, 71, 100, 332, 299, 22, 299, 336, 402, 209, 185, 37], [66, 67, 102, 135, 9, 440, 0, 66, 568, 569, 10, 274, 110, 111, 7, 502, 271, 35, 10, 274, 18, 390, 265, 134, 530, 37], [0, 570, 571, 10, 292, 18, 572, 573, 83, 13, 7, 544, 574, 558, 323, 18, 155, 22, 77, 575, 18, 572, 573, 83, 13, 230, 572, 573, 22, 118, 248, 249, 250, 456, 467, 37], [0, 33, 2, 325, 172, 13, 325, 457, 18, 576, 577, 13, 221, 578, 336, 402, 348, 579, 18, 34, 271, 13, 580, 185, 580, 397, 581, 91, 582, 60, 83, 18, 283, 531, 13, 397, 581, 91, 41, 540, 328, 125, 18, 567, 583, 101, 531, 583, 37], [0, 33, 2, 66, 28, 248, 248, 249, 250, 13, 135, 298, 7, 11, 35, 252, 253, 66, 299, 224, 127, 37], [0, 33, 41, 211, 11, 35, 155, 10, 292, 7, 584, 470, 18, 13, 238, 7, 33, 68, 154, 396, 585, 185, 469, 37], [0, 8, 318, 87, 586, 41, 540, 155, 567, 212, 250, 41, 540, 58, 146, 102, 13, 266, 587, 176, 177, 93, 588, 62], [0, 132, 33, 2, 160, 125, 542, 18, 360, 502, 13, 63, 7, 8, 71, 64, 589, 18, 545, 590, 13, 242, 193, 314, 7, 8, 71, 591, 592, 593, 38, 18, 545, 590, 37], [0, 360, 124, 13, 176, 177, 94, 22, 594, 135, 136, 155, 440, 232, 62], [109, 188, 161, 94, 228, 595, 185, 266, 22, 7, 596, 228, 445, 517, 125, 597, 101, 135, 136, 598, 595, 224, 62], [0, 566, 74, 14, 22, 226, 599, 13, 600, 148, 601, 22, 464, 602, 13, 603, 38, 457, 22, 226, 604, 13, 309, 605, 584, 22, 606, 607, 13, 323, 608, 22, 609, 31, 224, 13, 610, 611, 22, 152, 31, 583, 13, 22, 612, 613, 299, 609, 614, 615, 347, 84, 37], [0, 8, 71, 1, 22, 616, 35, 176, 177, 439, 95, 13, 349, 25, 617, 8, 459, 501, 35, 41, 540, 13, 102, 8, 318, 618, 253, 283, 619, 13, 109, 373, 91, 620, 621, 68, 69, 18, 477, 304, 101, 5, 6, 13, 503, 93, 68, 69, 18, 622, 623, 299, 457, 257, 102, 161, 462, 37], [0, 66, 387, 103, 295, 296, 624, 325, 18, 111, 75, 11, 35, 501, 250, 41, 540, 13, 230, 374, 375, 617, 87, 501, 37], [0, 158, 102, 35, 253, 224, 18, 336, 402, 13, 625, 25, 181, 34, 35, 18, 626, 627, 37], [0, 33, 2, 109, 177, 160, 13, 275, 1, 184, 177, 305, 33, 2, 62], [0, 33, 48, 25, 13, 242, 71, 1, 206, 628, 337, 209, 185, 13, 325, 172, 18, 621, 576, 21, 398, 66, 37], [0, 33, 179, 73, 629, 109, 177, 630, 118, 13, 132, 73, 629, 296, 102, 22, 391, 631, 632, 37], [0, 10, 292, 161, 263, 125, 18, 135, 298, 7, 8, 71, 32, 342, 36, 633, 13, 634, 635, 33, 2, 18, 263, 125, 299, 342, 37], [0, 636, 637, 7, 152, 127, 18, 13, 77, 269, 7, 130, 1, 18, 37], [0, 638, 223, 419, 60, 7, 638, 223, 5, 89, 382, 172, 18, 419, 60, 13, 68, 638, 223, 113, 295, 612, 185, 8, 639, 489, 535, 37], [0, 70, 185, 550, 618, 584, 449, 640, 13, 144, 323, 641, 634, 78, 642, 428, 37], [0, 38, 235, 273, 643, 174, 172, 235, 13, 23, 644, 349, 7, 645, 294, 646, 32, 647, 648, 36, 18, 100, 335, 13, 33, 7, 71, 179, 9, 649, 37], [0, 109, 509, 308, 650, 257, 398, 66, 13, 518, 295, 296, 145, 257, 135, 651, 37], [0, 33, 2, 18, 652, 111, 132, 336, 201, 653, 654, 253, 347, 130, 13, 230, 132, 655, 491, 224, 656, 633, 37], [0, 657, 332, 155, 658, 659, 101, 34, 491, 660, 250, 661, 662, 663, 174, 561, 499, 664, 665, 18, 456, 666, 13, 109, 459, 456, 666, 489, 667, 8, 270, 668, 669, 13, 43, 670, 671, 8, 270, 101, 672, 37], [0, 325, 172, 415, 673, 563, 674, 653, 675, 18, 676, 350, 101, 71, 1, 18, 677, 148, 468, 104, 13, 407, 537, 229, 84, 101, 71, 1, 22, 25, 678, 679, 563, 674, 653, 675, 18, 676, 350, 101, 71, 1, 680, 148, 681, 102, 18, 675, 682, 133, 102, 468, 101, 31, 63, 677, 148, 468, 104, 37], [0, 683, 149, 13, 33, 7, 87, 100, 598, 684, 18, 685, 686, 687, 37], [0, 535, 509, 504, 505, 400, 401, 130, 13, 33, 325, 18, 688, 689, 350, 134, 7, 24, 595, 328, 338, 155, 690, 468, 328, 691, 18, 616, 212, 37], [0, 311, 142, 13, 33, 101, 593, 1, 160, 164, 692, 5, 693, 13, 420, 91, 22, 617, 32, 342, 116, 36, 37], [0, 33, 304, 102, 71, 68, 69, 18, 271, 452, 694, 13, 695, 318, 696, 697, 524, 698, 13, 380, 161, 386, 692, 224, 71, 699, 225, 13, 23, 7, 699, 225, 257, 584, 388, 37], [0, 315, 9, 233, 347, 133, 315, 13, 700, 9, 233, 347, 133, 701, 37], [0, 702, 233, 7, 325, 172, 476, 468, 18, 703, 330, 13, 58, 7, 273, 704, 705, 18, 291, 240, 133, 132, 37], [0, 230, 95, 113, 114, 7, 374, 375, 213, 453, 135, 651, 101, 174, 457, 495, 581, 161, 18, 34, 113, 114, 37], [0, 109, 7, 706, 33, 8, 5, 38, 15, 18, 11, 568, 77, 538, 13, 707, 118, 484, 172, 73, 629, 708, 145, 156, 709, 347, 440, 87, 37], [0, 702, 1, 420, 342, 476, 389, 408, 101, 702, 710, 505, 77, 110, 213, 101, 9, 711, 13, 224, 712, 101, 124, 712, 612, 713, 242, 712, 135, 136, 13, 702, 1, 347, 714, 640, 3, 715, 716, 389, 408, 717, 705, 29, 718, 389, 408, 13, 717, 389, 408, 718, 242, 719, 13, 717, 705, 29, 718, 242, 719, 37], [0, 124, 712, 437, 33, 32, 720, 389, 36, 13, 224, 712, 715, 33, 32, 721, 2, 438, 36, 13, 109, 392, 18, 32, 614, 398, 36, 541, 241, 1, 610, 22, 118, 37], [0, 224, 154, 13, 722, 722, 722, 722, 13, 157, 541, 7, 71, 723, 438, 247], [0, 157, 724, 304, 725, 34, 225, 726, 33, 13, 612, 78, 33, 727, 372, 728, 729, 230, 730, 152, 157, 18, 395, 648, 37], [0, 32, 731, 14, 732, 36, 109, 556, 733, 225, 13, 416, 201, 109, 731, 734, 735, 736, 7, 733, 478, 102, 347, 18, 37], [0, 157, 345, 33, 22, 25, 13, 345, 118, 33, 13, 157, 308, 737, 124, 738, 74, 34, 739, 37], [0, 273, 740, 649, 13, 157, 423, 424, 386, 548, 741, 716, 271, 742, 345, 71, 699, 225, 37], [0, 413, 414, 204, 265, 18, 324, 18, 22, 132, 91, 373, 156, 204, 265, 743, 95, 744, 745, 18, 746, 102, 101, 747, 134, 13, 14, 7, 603, 748, 485, 103, 559, 60, 265, 427, 612, 485, 749, 204, 265, 83, 137, 75, 342, 290, 18, 204, 265, 37], [0, 77, 538, 750, 404, 18, 528, 199, 693, 323, 7, 548, 180, 305, 18, 62]]\n",
            "[[1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1], [1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1], [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1], [1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1], [0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1], [1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1], [1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1], [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1], [1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], [1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1], [1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1], [0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1], [1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1], [1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1], [1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1], [1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1], [1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1], [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1], [1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1], [1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1]]\n",
            "RNN(\n",
            "  (CharEmbed): Embedding(5180, 30)\n",
            "  (rnn): LSTM(30, 300, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=600, out_features=128, bias=True)\n",
            "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
            "  )\n",
            "  (out): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (non_linear): LeakyReLU(negative_slope=0.01)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            " 0   2000    1.292\n",
            " 0   4000    0.987\n",
            " 0   6000    0.877\n",
            " 0   8000    0.819\n",
            " 0  10000    0.781\n",
            " 0  12000    0.752\n",
            " 0  14000    0.729\n",
            " 0  16000    0.708\n",
            " 0  18000    0.688\n",
            " 0  20000    0.670\n",
            " 0  22000    0.653\n",
            " 0  24000    0.636\n",
            " 0  26000    0.619\n",
            " 0  28000    0.604\n",
            " 0  30000    0.588\n",
            " 0  32000    0.574\n",
            " 0  34000    0.560\n",
            " 0  36000    0.547\n",
            " 0  38000    0.535\n",
            " 0  40000    0.524\n",
            " 0  42000    0.514\n",
            " 0  44000    0.504\n",
            " 0  46000    0.496\n",
            " 0  48000    0.487\n",
            " 0  50000    0.479\n",
            " 0  52000    0.471\n",
            " 0  54000    0.464\n",
            " 0  56000    0.457\n",
            " 0  58000    0.450\n",
            " 0  60000    0.444\n",
            " 0  62000    0.438\n",
            " 0  64000    0.433\n",
            " 0  66000    0.428\n",
            " 0  68000    0.422\n",
            " 0  70000    0.418\n",
            " 0  72000    0.413\n",
            " 0  74000    0.408\n",
            " 0  76000    0.404\n",
            " 0  78000    0.400\n",
            " 0  80000    0.396\n",
            " 0  82000    0.392\n",
            " 0  84000    0.388\n",
            " 0  86000    0.385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 1   2000    0.356\n",
            " 1   4000    0.334\n",
            " 1   6000    0.315\n",
            " 1   8000    0.297\n",
            " 1  10000    0.283\n",
            " 1  12000    0.270\n",
            " 1  14000    0.258\n",
            " 1  16000    0.248\n",
            " 1  18000    0.238\n",
            " 1  20000    0.230\n",
            " 1  22000    0.222\n",
            " 1  24000    0.215\n",
            " 1  26000    0.208\n",
            " 1  28000    0.202\n",
            " 1  30000    0.197\n",
            " 1  32000    0.191\n",
            " 1  34000    0.187\n",
            " 1  36000    0.182\n",
            " 1  38000    0.178\n",
            " 1  40000    0.174\n",
            " 1  42000    0.170\n",
            " 1  44000    0.166\n",
            " 1  46000    0.163\n",
            " 1  48000    0.160\n",
            " 1  50000    0.157\n",
            " 1  52000    0.154\n",
            " 1  54000    0.151\n",
            " 1  56000    0.149\n",
            " 1  58000    0.146\n",
            " 1  60000    0.144\n",
            " 1  62000    0.142\n",
            " 1  64000    0.140\n",
            " 1  66000    0.138\n",
            " 1  68000    0.136\n",
            " 1  70000    0.134\n",
            " 1  72000    0.132\n",
            " 1  74000    0.130\n",
            " 1  76000    0.129\n",
            " 1  78000    0.127\n",
            " 1  80000    0.125\n",
            " 1  82000    0.124\n",
            " 1  84000    0.123\n",
            " 1  86000    0.121\n",
            " 2   2000    0.115\n",
            " 2   4000    0.110\n",
            " 2   6000    0.105\n",
            " 2   8000    0.101\n",
            " 2  10000    0.097\n",
            " 2  12000    0.093\n",
            " 2  14000    0.090\n",
            " 2  16000    0.087\n",
            " 2  18000    0.084\n",
            " 2  20000    0.082\n",
            " 2  22000    0.079\n",
            " 2  24000    0.077\n",
            " 2  26000    0.075\n",
            " 2  28000    0.073\n",
            " 2  30000    0.071\n",
            " 2  32000    0.070\n",
            " 2  34000    0.068\n",
            " 2  36000    0.066\n",
            " 2  38000    0.065\n",
            " 2  40000    0.064\n",
            " 2  42000    0.062\n",
            " 2  44000    0.061\n",
            " 2  46000    0.060\n",
            " 2  48000    0.059\n",
            " 2  50000    0.058\n",
            " 2  52000    0.057\n",
            " 2  54000    0.056\n",
            " 2  56000    0.055\n",
            " 2  58000    0.054\n",
            " 2  60000    0.053\n",
            " 2  62000    0.052\n",
            " 2  64000    0.051\n",
            " 2  66000    0.051\n",
            " 2  68000    0.050\n",
            " 2  70000    0.049\n",
            " 2  72000    0.048\n",
            " 2  74000    0.048\n",
            " 2  76000    0.047\n",
            " 2  78000    0.047\n",
            " 2  80000    0.046\n",
            " 2  82000    0.045\n",
            " 2  84000    0.045\n",
            " 2  86000    0.044\n",
            " 3   2000    0.044\n",
            " 3   4000    0.043\n",
            " 3   6000    0.043\n",
            " 3   8000    0.042\n",
            " 3  10000    0.042\n",
            " 3  12000    0.041\n",
            " 3  14000    0.041\n",
            " 3  16000    0.040\n",
            " 3  18000    0.040\n",
            " 3  20000    0.039\n",
            " 3  22000    0.039\n",
            " 3  24000    0.039\n",
            " 3  26000    0.038\n",
            " 3  28000    0.038\n",
            " 3  30000    0.038\n",
            " 3  32000    0.037\n",
            " 3  34000    0.037\n",
            " 3  36000    0.037\n",
            " 3  38000    0.036\n",
            " 3  40000    0.036\n",
            " 3  42000    0.036\n",
            " 3  44000    0.035\n",
            " 3  46000    0.035\n",
            " 3  48000    0.035\n",
            " 3  50000    0.034\n",
            " 3  52000    0.034\n",
            " 3  54000    0.034\n",
            " 3  56000    0.034\n",
            " 3  58000    0.033\n",
            " 3  60000    0.033\n",
            " 3  62000    0.033\n",
            " 3  64000    0.033\n",
            " 3  66000    0.032\n",
            " 3  68000    0.032\n",
            " 3  70000    0.032\n",
            " 3  72000    0.032\n",
            " 3  74000    0.031\n",
            " 3  76000    0.031\n",
            " 3  78000    0.031\n",
            " 3  80000    0.031\n",
            " 3  82000    0.031\n",
            " 3  84000    0.030\n",
            " 3  86000    0.030\n",
            " 4   2000    0.030\n",
            " 4   4000    0.030\n",
            " 4   6000    0.030\n",
            " 4   8000    0.029\n",
            " 4  10000    0.029\n",
            " 4  12000    0.029\n",
            " 4  14000    0.029\n",
            " 4  16000    0.029\n",
            " 4  18000    0.029\n",
            " 4  20000    0.028\n",
            " 4  22000    0.028\n",
            " 4  24000    0.028\n",
            " 4  26000    0.028\n",
            " 4  28000    0.028\n",
            " 4  30000    0.028\n",
            " 4  32000    0.027\n",
            " 4  34000    0.027\n",
            " 4  36000    0.027\n",
            " 4  38000    0.027\n",
            " 4  40000    0.027\n",
            " 4  42000    0.027\n",
            " 4  44000    0.027\n",
            " 4  46000    0.026\n",
            " 4  48000    0.026\n",
            " 4  50000    0.026\n",
            " 4  52000    0.026\n",
            " 4  54000    0.026\n",
            " 4  56000    0.026\n",
            " 4  58000    0.026\n",
            " 4  60000    0.026\n",
            " 4  62000    0.025\n",
            " 4  64000    0.025\n",
            " 4  66000    0.025\n",
            " 4  68000    0.025\n",
            " 4  70000    0.025\n",
            " 4  72000    0.025\n",
            " 4  74000    0.025\n",
            " 4  76000    0.025\n",
            " 4  78000    0.025\n",
            " 4  80000    0.024\n",
            " 4  82000    0.024\n",
            " 4  84000    0.024\n",
            " 4  86000    0.024\n",
            " 5   2000    0.024\n",
            " 5   4000    0.024\n",
            " 5   6000    0.024\n",
            " 5   8000    0.024\n",
            " 5  10000    0.024\n",
            " 5  12000    0.023\n",
            " 5  14000    0.023\n",
            " 5  16000    0.023\n",
            " 5  18000    0.023\n",
            " 5  20000    0.023\n",
            " 5  22000    0.023\n",
            " 5  24000    0.023\n",
            " 5  26000    0.023\n",
            " 5  28000    0.023\n",
            " 5  30000    0.023\n",
            " 5  32000    0.023\n",
            " 5  34000    0.022\n",
            " 5  36000    0.022\n",
            " 5  38000    0.022\n",
            " 5  40000    0.022\n",
            " 5  42000    0.022\n",
            " 5  44000    0.022\n",
            " 5  46000    0.022\n",
            " 5  48000    0.022\n",
            " 5  50000    0.022\n",
            " 5  52000    0.022\n",
            " 5  54000    0.022\n",
            " 5  56000    0.022\n",
            " 5  58000    0.021\n",
            " 5  60000    0.021\n",
            " 5  62000    0.021\n",
            " 5  64000    0.021\n",
            " 5  66000    0.021\n",
            " 5  68000    0.021\n",
            " 5  70000    0.021\n",
            " 5  72000    0.021\n",
            " 5  74000    0.021\n",
            " 5  76000    0.021\n",
            " 5  78000    0.021\n",
            " 5  80000    0.021\n",
            " 5  82000    0.021\n",
            " 5  84000    0.021\n",
            " 5  86000    0.020\n",
            " 6   2000    0.020\n",
            " 6   4000    0.020\n",
            " 6   6000    0.020\n",
            " 6   8000    0.020\n",
            " 6  10000    0.020\n",
            " 6  12000    0.020\n",
            " 6  14000    0.020\n",
            " 6  16000    0.020\n",
            " 6  18000    0.020\n",
            " 6  20000    0.020\n",
            " 6  22000    0.020\n",
            " 6  24000    0.020\n",
            " 6  26000    0.020\n",
            " 6  28000    0.020\n",
            " 6  30000    0.019\n",
            " 6  32000    0.019\n",
            " 6  34000    0.019\n",
            " 6  36000    0.019\n",
            " 6  38000    0.019\n",
            " 6  40000    0.019\n",
            " 6  42000    0.019\n",
            " 6  44000    0.019\n",
            " 6  46000    0.019\n",
            " 6  48000    0.019\n",
            " 6  50000    0.019\n",
            " 6  52000    0.019\n",
            " 6  54000    0.019\n",
            " 6  56000    0.019\n",
            " 6  58000    0.019\n",
            " 6  60000    0.019\n",
            " 6  62000    0.019\n",
            " 6  64000    0.019\n",
            " 6  66000    0.019\n",
            " 6  68000    0.018\n",
            " 6  70000    0.018\n",
            " 6  72000    0.018\n",
            " 6  74000    0.018\n",
            " 6  76000    0.018\n",
            " 6  78000    0.018\n",
            " 6  80000    0.018\n",
            " 6  82000    0.018\n",
            " 6  84000    0.018\n",
            " 6  86000    0.018\n",
            " 7   2000    0.018\n",
            " 7   4000    0.018\n",
            " 7   6000    0.018\n",
            " 7   8000    0.018\n",
            " 7  10000    0.018\n",
            " 7  12000    0.018\n",
            " 7  14000    0.018\n",
            " 7  16000    0.018\n",
            " 7  18000    0.018\n",
            " 7  20000    0.018\n",
            " 7  22000    0.017\n",
            " 7  24000    0.017\n",
            " 7  26000    0.017\n",
            " 7  28000    0.017\n",
            " 7  30000    0.017\n",
            " 7  32000    0.017\n",
            " 7  34000    0.017\n",
            " 7  36000    0.017\n",
            " 7  38000    0.017\n",
            " 7  40000    0.017\n",
            " 7  42000    0.017\n",
            " 7  44000    0.017\n",
            " 7  46000    0.017\n",
            " 7  48000    0.017\n",
            " 7  50000    0.017\n",
            " 7  52000    0.017\n",
            " 7  54000    0.017\n",
            " 7  56000    0.017\n",
            " 7  58000    0.017\n",
            " 7  60000    0.017\n",
            " 7  62000    0.017\n",
            " 7  64000    0.017\n",
            " 7  66000    0.017\n",
            " 7  68000    0.017\n",
            " 7  70000    0.017\n",
            " 7  72000    0.016\n",
            " 7  74000    0.016\n",
            " 7  76000    0.016\n",
            " 7  78000    0.016\n",
            " 7  80000    0.016\n",
            " 7  82000    0.016\n",
            " 7  84000    0.016\n",
            " 7  86000    0.016\n",
            " 8   2000    0.016\n",
            " 8   4000    0.016\n",
            " 8   6000    0.016\n",
            " 8   8000    0.016\n",
            " 8  10000    0.016\n",
            " 8  12000    0.016\n",
            " 8  14000    0.016\n",
            " 8  16000    0.016\n",
            " 8  18000    0.016\n",
            " 8  20000    0.016\n",
            " 8  22000    0.016\n",
            " 8  24000    0.016\n",
            " 8  26000    0.016\n",
            " 8  28000    0.016\n",
            " 8  30000    0.016\n",
            " 8  32000    0.016\n",
            " 8  34000    0.016\n",
            " 8  36000    0.016\n",
            " 8  38000    0.016\n",
            " 8  40000    0.015\n",
            " 8  42000    0.015\n",
            " 8  44000    0.015\n",
            " 8  46000    0.015\n",
            " 8  48000    0.015\n",
            " 8  50000    0.015\n",
            " 8  52000    0.015\n",
            " 8  54000    0.015\n",
            " 8  56000    0.015\n",
            " 8  58000    0.015\n",
            " 8  60000    0.015\n",
            " 8  62000    0.015\n",
            " 8  64000    0.015\n",
            " 8  66000    0.015\n",
            " 8  68000    0.015\n",
            " 8  70000    0.015\n",
            " 8  72000    0.015\n",
            " 8  74000    0.015\n",
            " 8  76000    0.015\n",
            " 8  78000    0.015\n",
            " 8  80000    0.015\n",
            " 8  82000    0.015\n",
            " 8  84000    0.015\n",
            " 8  86000    0.015\n",
            " 9   2000    0.015\n",
            " 9   4000    0.015\n",
            " 9   6000    0.015\n",
            " 9   8000    0.015\n",
            " 9  10000    0.015\n",
            " 9  12000    0.015\n",
            " 9  14000    0.015\n",
            " 9  16000    0.015\n",
            " 9  18000    0.015\n",
            " 9  20000    0.014\n",
            " 9  22000    0.014\n",
            " 9  24000    0.014\n",
            " 9  26000    0.014\n",
            " 9  28000    0.014\n",
            " 9  30000    0.014\n",
            " 9  32000    0.014\n",
            " 9  34000    0.014\n",
            " 9  36000    0.014\n",
            " 9  38000    0.014\n",
            " 9  40000    0.014\n",
            " 9  42000    0.014\n",
            " 9  44000    0.014\n",
            " 9  46000    0.014\n",
            " 9  48000    0.014\n",
            " 9  50000    0.014\n",
            " 9  52000    0.014\n",
            " 9  54000    0.014\n",
            " 9  56000    0.014\n",
            " 9  58000    0.014\n",
            " 9  60000    0.014\n",
            " 9  62000    0.014\n",
            " 9  64000    0.014\n",
            " 9  66000    0.014\n",
            " 9  68000    0.014\n",
            " 9  70000    0.014\n",
            " 9  72000    0.014\n",
            " 9  74000    0.014\n",
            " 9  76000    0.014\n",
            " 9  78000    0.014\n",
            " 9  80000    0.014\n",
            " 9  82000    0.014\n",
            " 9  84000    0.014\n",
            " 9  86000    0.014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8QJw2TIKNHS",
        "colab_type": "code",
        "outputId": "3a2a67ad-69b8-4d13-c810-187ded994d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# model = torch.load(\"model.pth\")\n",
        "\n",
        "with open('test_input', 'r') as f:\n",
        "  corpus = f.readlines()\n",
        "with open('test_label', 'r') as f:\n",
        "  corpus_target = f.readlines()\n",
        "for i in range(len(corpus)):\n",
        "  corpus[i] = corpus[i].replace('\\n','')\n",
        "  # corpus = ['扬帆远东做与中国合作的先行', '希腊的经济结构较特殊。', ...]\n",
        "  corpus_target[i] = corpus_target[i].replace('\\n', '')\n",
        "\n",
        "test_input = []\n",
        "for line in corpus:\n",
        "  conv_line = []\n",
        "  for c in line:\n",
        "    conv_line.append(converter.word_to_int(c))\n",
        "  test_input.append(conv_line)\n",
        "print(test_input[:100])\n",
        "\n",
        "test_tar = []\n",
        "for line in corpus_target:\n",
        "  conv_line = []\n",
        "  for c in line:\n",
        "    conv_line.append(int(c))\n",
        "  test_tar.append(conv_line)\n",
        "print(test_tar[:100])\n",
        "\n",
        "model.eval()\n",
        "def out_to_class(out):\n",
        "  m = nn.Softmax(2)\n",
        "  a = m(out).cpu()\n",
        "  a= a.detach().numpy()\n",
        "  return np.argmax(a,axis=-1)[0]\n",
        "\n",
        "err_count  =0 \n",
        "total_count = 0\n",
        "for i,test_text in enumerate(test_input):\n",
        "  data = torch.tensor([test_text]) \n",
        "  data = cuda(data)\n",
        "  out, ht = model(data)\n",
        "  answer = out_to_class(out)\n",
        "  l = len(answer)\n",
        "  total_count += l\n",
        "  local =0\n",
        "  for j in range(l):\n",
        "    \n",
        "    if test_tar[i][j] != answer[j]:\n",
        "      local+=1\n",
        "      err_count +=  1\n",
        "      if local/l>0.2:\n",
        "        print (corpus[i] )\n",
        "        print( [ (corpus[i][k], answer[k])  if answer[k]!= test_tar[i][k] else answer[k]  for k in range(l) ] )\n",
        "        \n",
        "        break\n",
        "      \n",
        "\n",
        "    \n",
        "\n",
        "print(err_count)\n",
        "print(total_count, err_count/total_count)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[565, 2442, 558, 53, 160, 16, 502, 325, 677, 181, 18, 111, 342], [291, 2833, 18, 221, 578, 498, 505, 272, 140, 1186, 37], [44, 419, 95, 1233, 3003, 518, 418, 347, 110, 13, 1116, 1939, 84, 1013, 746, 295, 296, 349, 567, 18, 1927, 1925, 1926, 37], [196, 87, 873, 874, 155, 1537, 2231, 58, 7, 221, 578, 103, 1120, 18, 398, 66, 1073, 77, 9, 328, 13, 134, 653, 95, 987, 1118, 449, 373, 272, 271, 37], [24, 475, 185, 13, 502, 291, 1203, 465, 142, 850, 462, 91, 272, 516, 18, 490, 378, 13, 291, 2833, 484, 1100, 146, 102, 132, 502, 325, 204, 265, 37], [939, 484, 475, 185, 13, 400, 401, 311, 496, 18, 502, 325, 221, 578, 93, 1747, 336, 402, 13, 558, 53, 132, 2635, 209, 37], [1923, 54, 235, 2130, 18, 1443, 158, 502, 102, 1924, 1918, 1926, 686, 294, 558, 53, 13, 47, 71, 215, 484, 1100, 145, 102, 119, 1243, 167, 1443, 521, 471, 502, 325, 1949, 684, 37], [63, 115, 610, 228, 118, 502, 325, 221, 578, 336, 402, 18, 34, 1910, 37], [63, 66, 16, 502, 325, 1, 677, 181, 37], [63, 185, 228, 502, 325, 13, 77, 78, 1253, 8, 71, 1277, 1627, 18, 34, 1443, 81, 37], [1277, 113, 851, 185, 13, 63, 373, 502, 325, 336, 402, 591, 592, 635, 38, 13, 63, 294, 291, 2833, 44, 419, 9, 213, 1892, 1893, 118, 116, 1099, 13, 503, 73, 118, 119, 325, 132, 44, 419, 155, 653, 1443, 95, 335, 293, 677, 181, 18, 456, 650, 37], [1927, 1928, 1928, 1920, 475, 1927, 1918, 215, 13, 291, 2833, 44, 419, 9, 213, 1277, 1627, 193, 13, 63, 977, 836, 0, 1443, 213, 297, 18, 456, 650, 16, 502, 335, 1952, 1248, 118, 132, 44, 419, 155, 653, 1443, 335, 293, 677, 181, 18, 23, 299, 16, 838, 1372, 37], [0, 1443, 213, 297, 205, 1, 266, 16, 484, 71, 1443, 81, 1361, 677, 209, 185, 106, 383, 16, 33, 558, 1405, 445, 446, 456, 306, 677, 265, 743, 95, 37], [0, 1443, 213, 297, 3, 4, 13, 66, 177, 22, 179, 13, 179, 230, 66, 20, 1253, 8, 37], [63, 303, 102, 295, 296, 380, 34, 18, 602, 1, 676, 692, 1261, 1443, 233, 13, 58, 66, 160, 16, 502, 325, 677, 181, 18, 111, 342, 37], [707, 106, 1, 5, 199, 625, 18, 1369, 993], [1940, 477, 3116, 13, 1128, 13, 1927, 1928, 1924, 1928, 475, 1927, 1918, 215, 5, 37], [1213, 1898, 34, 35, 708, 308, 35, 136, 10, 2105, 155, 560, 76, 5, 573, 274, 155, 136, 81, 407, 13, 502, 325, 708, 308, 35, 308, 30, 308, 213, 155, 502, 325, 708, 308, 390, 181, 10, 292, 451, 308, 30, 308, 213, 1139, 1599, 12, 213, 37], [24, 475, 185, 13, 8, 639, 70, 94, 1205, 876, 708, 308, 35, 155, 1073, 1074, 708, 308, 35, 335, 293, 18, 568, 569, 13, 81, 66, 1533, 1291, 102, 494, 708, 308, 35, 2327, 1088, 500, 522, 677, 1686, 526, 155, 494, 221, 578, 350, 134, 400, 401, 373, 1205, 876, 708, 308, 135, 136, 18, 49, 393, 500, 534, 37], [2179, 1533, 102, 494, 708, 308, 616, 212, 500, 155, 494, 1, 18, 130, 826, 500, 37], [381, 484, 475, 185, 13, 1139, 70, 94, 708, 308, 390, 181, 713, 708, 308, 415, 1250, 113, 114, 568, 569, 37], [1036, 719, 576, 631, 632, 791], [475, 793, 1, 1473, 67, 68, 33, 336, 402, 13, 453, 240, 28, 248, 68, 33, 199, 625, 7, 8, 168, 161, 94, 116, 13, 238, 184, 392, 163, 437, 28, 248, 68, 33, 199, 625, 175, 62], [33, 259, 78, 1, 5, 199, 625, 18, 28, 248, 423, 424, 102, 119, 335, 293, 18, 976, 464, 791, 8, 7, 71, 1, 373, 708, 308, 18, 182, 90, 13, 1254, 7, 708, 308, 373, 71, 1, 18, 614, 398, 101, 592, 417, 37], [109, 119, 318, 7, 449, 2649, 449, 77, 18, 37], [474, 475, 1, 132, 27, 404, 68, 69, 18, 1, 5, 1369, 993, 193, 13, 8, 404, 66, 707, 228, 1, 5, 199, 625, 101, 708, 308, 199, 625, 18, 980, 677, 318, 37], [314, 228, 118, 1602, 95, 5, 707, 390, 181, 18, 193, 194, 13, 257, 24, 205, 11, 5, 155, 568, 569, 5, 145, 291, 240, 75, 325, 172, 34, 504, 135, 101, 624, 188, 34, 445, 446, 13, 109, 205, 237, 23, 1287, 882, 37], [109, 459, 229, 84, 209, 318, 93, 13, 1586, 891, 161, 13, 8, 1593, 102, 504, 308, 23, 612, 179, 73, 8, 1367, 34, 94, 95, 37], [238, 33, 2, 58, 374, 375, 305, 228, 13, 109, 459, 229, 84, 18, 84, 1093, 454, 205, 478, 592, 13, 257, 24, 35, 5, 307, 897, 328, 1014, 75, 127, 13, 58, 97, 374, 299, 336, 1368, 68, 69, 18, 163, 179, 13, 237, 536, 502, 536, 77, 118, 8, 188, 1, 457, 265, 557, 18, 1621, 448, 37], [484, 475, 124, 185, 13, 547, 133, 35, 18, 41, 540, 951, 488, 118, 13, 547, 153, 102, 18, 914, 3301, 794, 146, 118, 37], [708, 308, 373, 8, 71, 1, 68, 74, 199, 625, 18, 528, 199, 13, 137, 22, 132, 91, 63, 133, 462, 18, 389, 84, 18, 93, 516, 13, 14, 7, 305, 63, 228, 932, 78, 708, 308, 160, 118, 176, 177, 102, 199, 625, 18, 94, 116, 13, 7, 22, 7, 591, 328, 336, 1368, 118, 63, 18, 1164, 299, 13, 160, 118, 68, 69, 457, 133, 299, 713, 18, 94, 116, 37], [14, 181, 78, 71, 1, 13, 68, 69, 256, 231, 21, 1297, 1298, 68, 33, 199, 625, 7, 834, 25, 228, 118, 28, 248, 37], [33, 18, 8, 84, 35, 5, 1602, 95, 348, 146, 102, 352, 132, 1213, 1898, 13, 14, 7, 485, 2136, 1985, 8, 71, 302, 127, 390, 181, 13, 1137, 690, 311, 336, 332, 390, 95, 869, 18, 1247, 1223, 987, 1223, 13, 8, 475, 160, 124, 185, 13, 103, 964, 1089, 34, 13, 1007, 143, 115, 228, 133, 35, 41, 540, 478, 22, 1180, 98, 13, 701, 66, 398, 485, 35, 861, 986, 591, 41, 540, 37], [101, 165, 459, 328, 132, 34, 504, 135, 237, 148, 812, 402, 163, 1627, 18, 242, 35, 449, 337, 13, 109, 84, 242, 35, 6, 25, 21, 591, 28, 155, 21, 102, 199, 625, 37], [102, 18, 242, 35, 58, 782, 308, 547, 68, 33, 199, 625, 18, 28, 248, 101, 256, 144, 18, 708, 308, 528, 199, 350, 136, 1361, 136, 209, 185, 305, 37], [3463, 489, 699, 571, 224, 1943, 1943, 2163, 293, 18, 86, 476, 66, 687, 155, 708, 308, 29, 559, 155, 743, 95, 172, 155, 349, 221, 212, 13, 369, 464, 465, 2040, 218, 8, 71, 1, 18, 476, 289, 1076, 1137, 155, 960, 621, 746, 102, 823, 13, 238, 109, 188, 528, 199, 350, 136, 101, 248, 28, 7, 102, 1228, 1425, 18, 37], [1416, 31, 7, 640, 294, 77, 820, 18, 232, 22, 7, 378, 1558, 18, 13, 489, 242, 8, 167, 34, 1443, 78, 118, 1941, 342, 13, 374, 66, 193, 66, 24, 463, 8, 459, 887, 975, 13, 238, 70, 213, 558, 18, 363, 318, 305, 13, 109, 167, 975, 266, 7, 639, 18, 37], [119, 318, 347, 714, 639, 975, 380, 1191, 13, 238, 21, 24, 18, 1, 7, 463, 887, 975, 18, 37], [133, 612, 33, 221, 3, 1066, 1067, 33, 18, 35, 5, 2, 127, 170, 171, 18, 100, 335, 155, 127, 652, 325, 380, 701, 66, 18, 100, 335, 390, 181, 37], [257, 24, 77, 820, 18, 1, 145, 7, 70, 380, 454, 508, 160, 209, 18, 37], [70, 708, 308, 35, 18, 1286, 530, 227, 13, 8, 71, 1, 349, 7, 70, 68, 69, 299, 1624, 407, 18, 390, 181, 160, 209, 13, 8, 639, 160, 228, 68, 69, 850, 22, 299, 1624, 407, 18, 390, 181, 37], [708, 308, 133, 503, 1161, 18, 504, 308, 13, 1416, 31, 7, 160, 34, 94, 18, 504, 308, 7, 102, 1193, 18, 13, 238, 160, 8, 459, 457, 133, 299, 713, 18, 271, 94, 18, 504, 308, 442, 257, 24, 37], [102, 459, 34, 35, 5, 360, 93, 121, 516, 13, 22, 2293, 91, 160, 271, 94, 116, 37], [31, 28, 271, 94, 116, 661, 548, 1, 18, 199, 625, 308, 21, 156, 1, 305, 398, 37], [708, 308, 373, 71, 1, 18, 614, 398, 349, 7, 350, 248, 132, 157, 133, 1124, 653, 18, 199, 625, 502, 13, 14, 68, 33, 199, 625, 18, 28, 248, 205, 74, 230, 7, 708, 308, 373, 157, 18, 182, 90, 18, 485, 899, 13, 242, 193, 58, 423, 424, 7, 157, 18, 592, 417, 133, 132, 37], [1036, 719, 576, 631, 632, 13, 228, 1043, 1149, 390, 181, 169, 231, 171, 8, 459, 13, 137, 102, 1610, 545, 546, 18, 693, 321, 13, 238, 489, 538, 812, 402, 163, 299, 18, 487, 714, 257, 34, 13, 14, 1041, 299, 591, 328, 336, 1368, 133, 35, 439, 213, 13, 22, 2639, 8, 775, 37], [522, 1940, 477, 3116, 526], [1510, 396, 561, 1167, 102, 133, 181, 78], [1002, 1961, 631, 632, 791], [305, 118, 157, 18, 635, 13, 33, 156, 157, 635, 502, 559, 2163, 73, 18, 1736, 171, 155, 1488, 2808, 612, 713, 173, 980, 67, 365, 18, 116, 1578, 151, 60, 118, 37], [33, 132, 769, 3, 1686, 3066, 390, 181, 502, 13, 266, 103, 228, 22, 391, 285, 43, 157, 109, 392, 18, 474, 475, 1, 18, 185, 635, 37], [474, 475, 1, 68, 614, 161, 1624, 13, 585, 91, 75, 397, 13, 238, 312, 312, 610, 172, 1032, 155, 708, 308, 926, 824, 18, 4222, 3233, 13, 1, 5, 211, 232, 224, 891, 228, 18, 1309, 1267, 58, 24, 37], [1309, 1267, 24, 118, 13, 22, 299, 720, 27, 373, 1586, 13, 630, 14, 630, 347, 13, 343, 464, 465, 38, 2202, 693, 2247, 13, 8, 5168, 22, 1510, 37], [31, 28, 13, 87, 132, 18, 170, 584, 771, 171, 137, 22, 23, 372, 13, 23, 372, 18, 7, 132, 109, 8, 475, 1265, 945, 13, 22, 391, 474, 475, 1, 221, 610, 22, 209, 239, 1517, 16, 1309, 1267, 13, 380, 348, 68, 69, 346, 1517, 118, 68, 69, 37], [33, 109, 1515, 635, 237, 148, 548, 157, 525, 73, 8, 167, 793, 1648, 1458, 207, 18, 1, 5, 347, 232, 13, 158, 7, 304, 373, 157, 324, 130, 18, 771, 881, 38, 212, 181, 459, 328, 1320, 13, 137, 16, 157, 1952, 1248, 8, 459, 1173, 1026, 771, 824, 18, 335, 148, 37], [157, 635, 502, 259, 78, 7, 172, 1032, 18, 3428, 419, 155, 954, 934, 18, 926, 824, 747, 134, 174, 157, 13, 897, 157, 22, 299, 541, 720, 100, 5, 6, 13, 584, 612, 102, 133, 181, 78, 37], [157, 172, 1032, 18, 309, 891, 27, 28, 601, 1, 242, 116, 13, 238, 157, 58, 66, 305, 228, 13, 109, 188, 824, 891, 137, 22, 7, 157, 172, 138, 102, 18, 37], [33, 2, 47, 71, 1, 145, 66, 293, 405, 5, 131, 1241, 20, 13, 407, 537, 1, 158, 66, 6, 132, 295, 224, 13, 158, 66, 242, 31, 63, 1, 75, 342, 708, 308, 642, 312, 13, 230, 308, 891, 228, 449, 43, 18, 113, 114, 37], [264, 2282, 1, 739, 155, 5, 273, 246, 607, 155, 800, 706, 621, 1082, 145, 7, 248, 28, 843, 363, 5, 6, 502, 131, 132, 18, 13, 693, 323, 328, 803, 155, 978, 979, 895, 1054, 21, 7, 193, 3, 877, 520, 132, 1, 2, 523, 1342, 37], [545, 546, 18, 475, 645, 24, 246, 13, 821, 1152, 714, 18, 4998, 4999, 13, 239, 127, 590, 1, 18, 1874, 258, 13, 47, 71, 1, 717, 24, 717, 391, 145, 102, 350, 308, 37], [8, 71, 1, 146, 102, 891, 228, 109, 188, 22, 1310, 13, 58, 308, 891, 228, 196, 8, 188, 432, 1029, 37], [8, 71, 1, 132, 351, 168, 94, 224, 1458, 207, 8, 459, 13, 132, 196, 8, 168, 94, 224, 23, 299, 308, 483, 2028, 37], [22, 391, 1, 891, 228, 345, 346, 101, 1309, 1267, 193, 13, 1077, 833, 5, 6, 22, 445, 378, 37], [33, 2, 3, 3, 257, 584, 4, 1297, 1298, 5, 6, 502, 228, 932, 7, 171, 24, 266, 7, 671, 24, 13, 158, 7, 23, 612, 1413, 404, 171, 373, 1, 18, 561, 1167, 345, 346, 380, 34, 37], [693, 719, 1062, 577, 18, 1, 380, 321, 693, 186, 171, 13, 109, 23, 612, 917, 25, 275, 1, 18, 135, 651, 16, 242, 116, 13, 70, 14, 68, 33, 2633, 4083, 13, 707, 228, 237, 133, 94, 94, 18, 958, 684, 37], [157, 4, 13, 157, 18, 172, 1032, 18, 624, 188, 113, 114, 1602, 1055, 266, 146, 1374, 932, 495, 581, 13, 38, 116, 142, 850, 793, 1648, 22, 209, 185, 37], [534, 1586, 180, 185, 495, 581, 175, 62], [489, 538, 158, 7, 159, 113, 114, 851, 3748, 91, 275, 1, 155, 172, 1032, 13, 22, 160, 909, 457, 13, 22, 1499, 413, 414, 13, 158, 7, 174, 315, 155, 171, 191, 13, 157, 172, 18, 113, 114, 23, 299, 308, 580, 470, 580, 1597, 13, 157, 68, 69, 18, 824, 1099, 58, 230, 584, 612, 400, 432, 37], [16, 31, 1077, 264, 1416, 1, 13, 22, 489, 150, 610, 109, 188, 248, 28, 13, 720, 571, 109, 188, 248, 28, 13, 22, 897, 68, 69, 2670, 91, 31, 502, 13, 137, 98, 68, 69, 18, 971, 972, 16, 347, 75, 342, 394, 369, 1241, 20, 37], [157, 7, 71, 102, 1076, 1137, 18, 474, 475, 13, 161, 35, 224, 75, 13, 453, 1586, 174, 78, 708, 308, 24, 160, 182, 90, 13, 28, 248, 68, 69, 18, 199, 625, 37], [406, 91, 172, 1032, 624, 188, 978, 979, 18, 811, 457, 13, 157, 18, 976, 38, 239, 127, 378, 379, 13, 5, 6, 371, 118, 757, 148, 13, 22, 238, 153, 102, 18, 163, 179, 25, 22, 228, 812, 402, 13, 1164, 457, 22, 299, 591, 328, 1414, 1415, 13, 14, 1041, 89, 5, 118, 68, 33, 1309, 1517, 18, 68, 3884, 38, 212, 13, 239, 127, 118, 624, 188, 1178, 67, 18, 585, 819, 37], [66, 400, 432, 109, 188, 466, 293, 13, 110, 111, 66, 1510, 396, 561, 1167, 13, 22, 299, 273, 996, 38, 235, 213, 118, 773, 43, 18, 13, 1441, 1462, 13, 315, 2798, 13, 66, 897, 68, 69, 412, 39, 124, 185, 13, 956, 404, 124, 185, 13, 35, 308, 81, 421, 68, 69, 13, 432, 87, 9, 747, 134, 78, 976, 9, 747, 134, 37], [58, 782, 157, 8, 193, 304, 22, 73, 862, 148, 495, 581, 351, 8, 139, 350, 771, 584, 13, 238, 157, 66, 909, 457, 1173, 1026, 447, 369, 116, 1578, 13, 160, 161, 373, 458, 5, 6, 18, 106, 383, 37], [157, 18, 256, 408, 347, 315, 13, 7, 400, 432, 172, 1032, 18, 221, 578, 1319, 1099, 37], [129, 118, 475, 645, 18, 545, 590, 13, 157, 43, 1100, 386, 146, 102, 1110, 597, 13, 157, 230, 7, 172, 502, 18, 1128, 125, 1858, 155, 2010, 1360, 703, 13, 157, 423, 424, 693, 540, 228, 109, 8, 318, 13, 22, 66, 1119, 372, 13, 22, 66, 938, 606, 13, 585, 226, 100, 563, 1471, 209, 157, 18, 690, 407, 37], [367, 367, 934, 1498, 172, 502, 18, 64, 2174, 248, 28, 304, 862, 148, 13, 110, 111, 651, 693, 22, 548, 68, 69, 404, 124, 22, 248, 28, 18, 324, 993, 37], [423, 424, 720, 27, 100, 259, 540, 68, 69, 18, 299, 457, 13, 212, 971, 100, 328, 1320, 624, 188, 81, 843, 363, 399, 283, 13, 1134, 1134, 28, 28, 100, 134, 985, 1791, 1652, 40, 75, 155, 980, 28, 23, 342, 18, 1013, 1223, 13, 581, 22, 161, 93, 4400, 558, 155, 360, 93, 121, 516, 37], [242, 193, 66, 259, 540, 228, 13, 66, 179, 77, 8, 168, 94, 13, 78, 118, 547, 185, 18, 1710, 13, 8, 404, 66, 51, 360, 130, 18, 171, 13, 66, 102, 109, 71, 477, 304, 106, 383, 13, 102, 634, 404, 18, 693, 719, 37], [237, 1088, 483, 228, 176, 177, 771, 584, 101, 1309, 1267, 13, 145, 423, 503, 1730, 68, 69, 791, 33, 22, 299, 2243, 477, 371, 304, 13, 94, 116, 7, 102, 291, 240, 18, 13, 73, 248, 239, 866, 7, 908, 782, 18, 13, 1192, 193, 38, 1578, 22, 1270, 58, 7, 720, 3, 18, 13, 8, 980, 145, 308, 152, 127, 37], [66, 305, 228, 623, 161, 18, 130, 826, 13, 447, 369, 18, 693, 627, 158, 299, 98, 394, 369, 18, 155, 248, 28, 18, 342, 60, 185, 495, 129, 37], [157, 1461, 224, 118, 302, 235, 18, 405, 193, 640, 1967, 687, 13, 230, 423, 424, 207, 98, 109, 71, 504, 308, 161, 161, 179, 13, 24, 656, 459, 501, 757, 13, 24, 204, 459, 2347, 125, 13, 8, 185, 23, 612, 67, 25, 75, 8, 825, 336, 402, 13, 1254, 185, 58, 23, 1697, 459, 2347, 448, 986, 1064, 172, 98, 37], [157, 58, 23, 612, 102, 693, 540, 100, 35, 459, 1205, 95, 11, 252, 41, 540, 13, 700, 365, 545, 590, 188, 161, 200, 13, 109, 58, 7, 400, 896, 172, 1032, 1319, 1099, 18, 8, 167, 232, 125, 37], [349, 347, 13, 66, 1066, 209, 585, 819, 13, 22, 372, 239, 1517, 13, 34, 1433, 1709, 775, 13, 449, 635, 157, 644, 124, 18, 232, 308, 580, 463, 580, 1304, 1045, 37], [102, 84, 35, 83, 4, 152, 791, 0, 79, 1, 347, 133, 612, 79, 34, 13, 135, 298, 132, 91, 791, 256, 63, 16, 275, 1, 88, 462, 1622, 824, 193, 13, 275, 1, 239, 127, 212, 971, 13, 63, 878, 124, 581, 38, 28, 248, 68, 69, 18, 324, 993, 37, 297], [33, 2, 640, 152, 278, 1069, 794, 795, 68, 69, 13, 22, 998, 889, 619, 1681, 2906, 1309, 1267, 18, 299, 457, 13, 58, 23, 612, 77, 78, 5, 6, 502, 18, 576, 83, 37], [1002, 1961, 631, 632, 13, 321, 157, 713, 1232, 96, 164, 1447, 231, 155, 171, 191, 155, 461, 240, 18, 116, 1578, 13, 585, 226, 100, 150, 610, 5, 6, 18, 1461, 776, 13, 160, 8, 71, 561, 1167, 2813, 748, 155, 102, 133, 181, 78, 18, 474, 475, 37], [291, 240, 299, 849, 228, 157, 18, 161, 447, 1189, 37], [541, 116, 8, 491, 78, 1442, 326], [331, 53, 2973, 1526, 1163, 209, 2181, 956, 3132, 3130, 876, 1846, 542, 3132, 2095, 1611, 13, 939, 1904, 475, 489, 8, 769, 561, 38, 1275, 1051, 8, 84, 1442, 326, 273, 1, 18, 1029, 94, 13, 115, 60, 118, 1895, 302, 968, 956, 18, 302, 590, 2, 37], [1927, 1928, 1921, 1918, 475, 1908, 264, 18, 8, 71, 124, 1859, 13, 3132, 3130, 876, 475, 381, 1917, 1918, 214, 18, 1442, 2153, 273, 1, 3132, 1138, 172, 4502, 174, 1848, 2538, 806, 490, 37], [399, 612, 130, 605, 152, 474, 488, 360, 246, 13, 3132, 1138, 172, 18, 523, 360, 239, 201, 13, 1342, 360, 852, 576, 299, 305, 228, 381, 462, 18, 53, 54, 37], [475, 1123, 1927, 1917, 214, 18, 3132, 2095, 1611, 323, 1319, 13, 1254, 225, 146, 4, 13, 150, 152, 273, 1, 1617, 224, 18, 1471, 125, 13, 1649, 63, 806, 485, 172, 37], [3132, 1138, 172, 119, 714, 516, 3247, 18, 271, 919, 235, 13, 2581, 1854, 1227, 329, 13, 55, 71, 124, 644, 18, 100, 335, 145, 146, 102, 37], [2095, 1611, 314, 7, 1254, 225, 146, 4, 13, 1968, 1747, 547, 273, 1, 18, 156, 4792, 510, 161, 13, 159, 1387, 155, 1512, 155, 2548, 155, 1953, 2314, 118, 13, 547, 100, 293, 1650, 179, 1694, 13, 231, 348, 314, 159, 273, 1, 18, 1871, 2245, 164, 1076, 485, 172, 2314, 2579, 37], [256, 193, 13, 271, 3132, 343, 124, 118, 581, 38, 791, 8, 404, 66, 547, 273, 1, 2046, 600, 161, 13, 241, 63, 152, 224, 1039, 1558, 769, 125, 37], [612, 348, 13, 26, 47, 264, 145, 66, 228, 273, 1, 172, 235, 13, 345, 444, 155, 715, 1534, 155, 551, 293, 155, 806, 490, 155, 160, 1421, 155, 2314, 2245, 1006, 1006, 42, 1087, 13, 26, 230, 78, 273, 1, 1048, 161, 3522, 3954, 195, 22, 228, 1783, 264, 13, 26, 230, 548, 273, 1, 419, 127, 1954, 101, 2106, 105, 37], [3132, 1138, 172, 273, 1, 1629, 22, 1037, 205, 876, 18, 2810, 1658, 490, 13, 2095, 1611, 230, 2792, 174, 63, 127, 119, 445, 235, 87, 18, 209, 2181, 876, 806, 46, 1658, 490, 13, 639, 228, 205, 876, 58, 345, 118, 46, 490, 1658, 37], [977, 836, 381, 119, 475, 18, 150, 151, 13, 2095, 1611, 115, 228, 273, 1, 8, 71, 1, 132, 172, 193, 1440, 25, 1837, 13, 343, 1615, 261, 174, 548, 63, 759, 8, 71, 103, 670, 504, 37], [78, 118, 1604, 103, 670, 504, 18, 628, 13, 26, 132, 68, 69, 555, 918, 18, 1157, 224, 2110, 71, 1331, 125, 13, 47, 264, 1671, 1421, 348, 230, 4309, 2431, 751, 4389, 37], [165, 193, 13, 8, 1199, 2431, 751, 4389, 299, 695, 1891, 1286, 628, 37]]\n",
            "[[0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1], [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1], [0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1], [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1], [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 0, 1, 0, 1, 1, 0, 1], [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1], [0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1], [0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1], [0, 0, 1, 0, 1, 1], [0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1], [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1], [0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1], [0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1], [0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1], [0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1], [0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], [0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], [0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1], [0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1], [1, 0, 0, 1, 1], [0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1], [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1], [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1], [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1], [0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1], [1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1], [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1], [1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], [0, 1, 1, 1, 0, 1, 1, 1], [0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1], [0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1], [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1], [0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1], [0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1], [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1], [0, 1, 0, 1, 1, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], [0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1], [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1], [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1]]\n",
            "辛志强朋友：\n",
            "[0, ('志', 1), ('强', 0), 0, 1, 1]\n",
            "有些大学生眼高手低，不屑于做小事情。\n",
            "[0, 1, 0, 0, ('生', 0), ('眼', 1), 0, ('手', 1), 1, 1, ('不', 1), 0, 1, 1, 1, 0, 1, 1]\n",
            "玲英思前想后，对哥哥说：“我不忍心扔下穆大爷不管啊！”\n",
            "[0, ('英', 0), ('思', 1), ('前', 1), ('想', 1), 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, ('扔', 1), ('下', 0), 1, 0, 1, 0, 1, 1, 1, 1]\n",
            "山西壶关县紫团山农民通讯组秦凤鸣代笔\n",
            "[0, 1, 0, 0, 1, 0, 0, 0, 0, ('民', 1), 0, ('讯', 1), 1, 0, 0, ('鸣', 0), ('代', 1), 1]\n",
            "嫁汉嫁汉，穿衣吃饭。\n",
            "[('嫁', 0), 1, ('嫁', 0), 1, 1, 1, 1, ('吃', 1), 1, 1]\n",
            "一代名舰沉没江底５８年后重展风采中山舰整体打捞出水\n",
            "[0, 1, ('名', 0), 1, ('沉', 1), 1, ('江', 0), 1, 0, 0, 1, 1, 1, 1, ('风', 1), 1, ('中', 1), ('山', 0), 1, 0, 1, 0, 1, ('出', 1), ('水', 0)]\n",
            "观测哨站沿淮建联防联治斩“黑龙”安徽治理淮河污染开始攻坚\n",
            "[0, ('测', 0), 1, 1, 1, ('淮', 0), 1, 0, 1, ('联', 0), 1, 1, ('“', 0), ('黑', 0), ('龙', 0), ('”', 0), 0, ('徽', 0), 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
            "陕甘宁气田至北京输气管道工程线路走向示意图（示意图：孙伟绘）\n",
            "[0, ('甘', 1), ('宁', 0), ('气', 1), 1, 1, 0, 1, ('输', 0), 1, 0, 1, 0, 1, 0, 1, ('走', 1), 1, 0, ('意', 1), 1, 1, 0, ('意', 1), 1, 1, 0, 1, 1, 1]\n",
            "观风测雨保平安（附图片１张）\n",
            "[('观', 1), ('风', 0), ('测', 0), 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n",
            "只待合龙\n",
            "[1, 1, 0, ('龙', 0)]\n",
            "如临深渊如履薄冰\n",
            "[0, ('临', 1), 0, 1, ('如', 1), 0, ('薄', 1), 1]\n",
            "四川机电提灌抗旱减灾显威力费县引导农民办好小水利\n",
            "[0, ('川', 0), 0, 1, 0, 1, 0, 1, 0, 1, ('显', 0), ('威', 1), ('力', 0), ('费', 1), 1, 0, 1, 0, 1, 1, ('好', 0), ('小', 0), 0, 1]\n",
            "温济泽\n",
            "[0, 0, ('泽', 0)]\n",
            "黄晴\n",
            "[('黄', 1), 1]\n",
            "海纳百川，有容乃大。\n",
            "[('海', 0), ('纳', 0), ('百', 0), 1, 1, ('有', 0), 1, 1, 1, 1]\n",
            "江泽民李鹏重要批示极大激励农科界人士\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 1, ('极', 1), 1, 0, 1, ('农', 0), ('科', 0), ('界', 0), 0, 1]\n",
            "①表示：（王＋佩右）\n",
            "[1, 0, 1, 1, 1, ('王', 0), ('＋', 0), ('佩', 0), 1, 1]\n",
            "关于康乾盛世。\n",
            "[0, 1, ('康', 0), ('乾', 0), 0, 1, 1]\n",
            "付款时总少不了一句：“请您点清。”\n",
            "[0, 1, 1, ('总', 0), ('少', 1), ('不', 1), 1, 1, 1, 1, 1, 1, 1, ('点', 1), 1, 1, 1]\n",
            "（寇占文杨海洋）\n",
            "[1, ('寇', 1), ('占', 1), 1, 0, 0, 1, 1]\n",
            "唱响国企志气歌－－焦作矿务局凝聚人心走出亏损泥沼\n",
            "[1, 1, 0, ('企', 0), 0, ('气', 0), 1, 1, 1, 0, 0, 0, 0, ('局', 0), 0, ('聚', 0), 0, 1, 0, 1, 0, ('损', 0), 0, ('沼', 0)]\n",
            "井陉：背水一战“生命渠”--河北太行山纪行之二\n",
            "[0, 1, 1, 0, ('水', 1), ('一', 1), 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, ('山', 0), 0, ('行', 0), ('之', 0), 1]\n",
            "本报记者何伟任之王尧\n",
            "[0, 1, 0, 1, 0, ('伟', 0), ('任', 1), 1, 0, ('尧', 0)]\n",
            "人民自有回天力。\n",
            "[0, 1, 1, 1, ('回', 1), ('天', 0), 1, 1]\n",
            "“淹要淹个明白。”\n",
            "[1, ('淹', 0), 1, ('淹', 0), 1, 0, 1, 1, 1]\n",
            "编者点评\n",
            "[0, 1, ('点', 1), ('评', 0)]\n",
            "本报记者任之王尧何伟\n",
            "[0, 1, 0, 1, ('任', 1), 1, 0, ('尧', 0), 0, ('伟', 0)]\n",
            "大灾过后，百废待兴。\n",
            "[('大', 0), 1, ('过', 1), 1, 1, 0, 0, ('待', 1), 1, 1]\n",
            "新疆：『一黑一白』交相辉映\n",
            "[0, 1, 1, 1, 1, 1, ('一', 0), 1, 1, ('交', 1), 0, ('辉', 1), 1]\n",
            "首期重点国有企业领导干部进修班结业胡锦涛为学员颁发证书\n",
            "[0, 1, 0, ('点', 1), 0, ('有', 1), 0, ('业', 1), 0, 0, 0, ('部', 1), 0, ('修', 1), 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, ('书', 0)]\n",
            "编者点评\n",
            "[0, 1, ('点', 1), ('评', 0)]\n",
            "要按质论价，不得压级压价。\n",
            "[1, ('按', 1), ('质', 1), 0, 1, 1, 0, 1, ('压', 0), 1, 0, 1, 1]\n",
            "被保险人负气自杀保险公司应否负责\n",
            "[1, 0, ('险', 0), ('人', 0), 0, ('气', 0), 0, ('杀', 0), 0, 0, 0, 1, 0, 1, 0, 1]\n",
            "（湖南攸县彭志文）\n",
            "[1, 0, ('南', 0), 0, ('县', 0), 0, 0, 1, 1]\n",
            "（３）棉田中后期。\n",
            "[1, 1, ('）', 0), 0, 1, ('中', 0), 0, 1, 1]\n",
            "贷款证缘何难推行？\n",
            "[0, 1, ('证', 0), ('缘', 1), ('何', 0), 1, 0, 1, 1]\n",
            "（中国人民银行河北徐水县支行田秀锦）\n",
            "[1, 0, 0, 0, 0, 0, ('行', 1), 0, ('北', 1), 0, 0, 0, 0, ('行', 0), 0, 0, ('锦', 0), 1]\n",
            "净化苏北种子市场\n",
            "[0, 1, ('苏', 0), 1, 0, ('子', 0), 0, 1]\n",
            "第二章营运管理\n",
            "[0, 0, ('章', 0), ('营', 1), ('运', 0), 0, 1]\n",
            "壮大中远集团繁荣香港经济\n",
            "[0, ('大', 0), 0, 0, 0, 1, 0, 1, 0, ('港', 0), 0, ('济', 0)]\n",
            "锻得新钢铸宝刀\n",
            "[1, 1, ('新', 0), 1, ('铸', 0), 0, ('刀', 0)]\n",
            "“胸怀祖国齐协作，志凌九霄铸银河”。\n",
            "[1, 0, 1, 0, 1, 1, 0, 1, 1, ('志', 0), ('凌', 0), 0, ('霄', 0), ('铸', 0), 0, 1, 1, 1]\n",
            "游景玉带领亚仿人今天又在以新的姿态向着新的高峰攀登。\n",
            "[0, ('景', 1), ('玉', 0), ('带', 1), ('领', 0), ('亚', 1), ('仿', 0), 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, ('向', 1), 1, 1, 1, 0, 1, 0, 1, 1]\n",
            "胡泳\n",
            "[0, ('泳', 0)]\n",
            "由启蒙期进入成长期\n",
            "[1, 0, ('蒙', 0), 1, 0, 1, 0, ('长', 0), 1]\n",
            "以库养库，探索数据库建设新路\n",
            "[1, 1, ('养', 0), 1, 1, 0, 1, 0, ('据', 1), 1, 0, 1, ('新', 0), ('路', 0)]\n",
            "步步领先再攻关\n",
            "[('步', 0), 1, 0, 1, ('再', 0), ('攻', 1), 1]\n",
            "科研无处不在\n",
            "[0, 1, ('无', 1), 1, ('不', 1), 1]\n",
            "高定彝\n",
            "[0, 0, ('彝', 0)]\n",
            "小疏忽引出大问题\n",
            "[1, ('疏', 1), 1, ('引', 1), 1, 1, 0, 1]\n",
            "信息包经过多台主机的传递，就可以到达目的地。\n",
            "[0, 1, ('包', 0), ('经', 1), 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, ('到', 1), ('达', 0), 0, ('的', 1), 1, 1]\n",
            "（摘自《中国青年报》邓泓文）\n",
            "[1, ('摘', 1), 1, 1, 0, 0, 0, 0, ('报', 0), ('》', 0), 0, ('泓', 0), 1, 1]\n",
            "桑新民\n",
            "[0, 0, ('民', 0)]\n",
            "写作方式的变革\n",
            "[('写', 1), 1, 0, 1, 1, 0, ('革', 0)]\n",
            "“换笔”引出的话题\n",
            "[1, ('换', 0), 1, 1, ('引', 1), 1, 1, 0, 1]\n",
            "｀得心应手＇大概就是这个意思吧。”\n",
            "[('｀', 0), ('得', 1), ('心', 1), ('应', 1), ('手', 0), 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]\n",
            "5703\n",
            "184355 0.030934881071845083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN21SFlZVDUP",
        "colab_type": "text"
      },
      "source": [
        "## Chinese Segmenter\n",
        "---\n",
        "1. Upload the given training and test corpus into the Colab notebook and run all the cells one by one gives us the result.\n",
        "2. The output from this neural net is a '1' and '0' representation of either each character has a space behind it.\n",
        "3. The loss function is nn.CrossEntropyLoss, which measures the accuracy of prediction.\n",
        "4. BiDirectional_RNN works better for this task. We need to see both before and after each character to know either to put a segmentor behind it or not.\n",
        "5. The result from my model after 10 epochs is 96.9%.\n",
        "6. May or may not help if we add traditional Chinese character data into the corpus. It may enlarge the different character list and cause extra learning load for the network, while the simplified counterpart of a traditional character may offer more information for a character. The embedding for traditional characters will probably be close to its simplified counterpart, for they often mean the same thing in context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTaUC7HqXT2s",
        "colab_type": "text"
      },
      "source": [
        "## Non-coding Problems - 1\n",
        "1. Model 1, CNN classifier:  kernel-size=3, six convolutional layers of size: 48, 48, 96, 96, 192, 192, with 2 hidden layers of size 512 and 256; Number of parameters is 3 * 3 * (3 * 48+ 48 * 48+ 48 * 96+ 96 * 96 + 96 * 192+192 * 192) + 4 * 4 * 192 * 512 + 512 * 256 + 256 * 10 = 2,350,608\n",
        "<p> Model 2, VGG16: 15,106,240\n",
        "<p> Model 3, Auto-Encoder: 3 * 3 *( 3 * 3 + 3 * 3) * 2 = 324\n",
        "2. Model 1, Char-Generator: take the GRU case for example, (3 * hidden_size * input_size + 3 * hidden_size * hidden_size) * layer_1, (3 * hidden_size * hidden_size + 3 * hidden_size * hidden_size) * layer_1, embedding 3601 * 300, plus fully connected layers, 1,460,742 in total.\n",
        "<p> Model 2, Chinese-segmentor: Embed(5180 * 30) + biLSTM(input_size=30, hidden_size=300, layers=2) + FFNN(600 * 128, 128 * 64, 64 * 128, 128 * 2); which is 1,040,840 in total.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7luwtg0bO56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d577f7a2-5510-4e86-fba5-4a2f23176078"
      },
      "source": [
        "3 * 3 * (3 * 48+ 48 * 48+ 48 * 96+ 96 * 96 + 96 * 192+192 * 192) + 4 * 4 * 192 * 512 + 512 * 256 + 256 * 10"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2350608"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nylqghIBcVAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2602a3b8-a615-4f08-8f19-db479d26a34d"
      },
      "source": [
        "3 * 3 *( 3 * 64 + 64 * 64 + 64 * 128 + 128 * 128 + 128 * 256 + 256 * 256 + 256 * 256 + 256 * 512 + 512 * 512 * 5 ) + 512 * 512 + 512 * 256 + 256 * 10 "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15106240"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQbntVbef2q_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95893565-5e33-487c-940d-6aa0ca351789"
      },
      "source": [
        "3601*30+(3*300*30+3*300*300+3*300*300*2)+300*128+128*64+64*128+128*3601"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1460742"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM_PFFrRhmR0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04a2dd1d-6268-41ed-f458-33b885fc9464"
      },
      "source": [
        "5180*30+(4*300*30+4*300*300)*2+600*128+128*64+64*128+128*2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1040840"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbZ9FFBGjwBG",
        "colab_type": "text"
      },
      "source": [
        "## Non-coding problems-2\n",
        "1. char-level model vs. word-level model:\n",
        "<p> pros: smaller model, freqency of each char is bigger\n",
        "<p> cons: longer to train, less meaning\n",
        "2. RNN vs. CNN :\n",
        "<p> pros: better for understanding sequential data\n",
        "<p> cons: deep and not able to train in parallel\n",
        "3. Make use of past and futrure info:\n",
        "<p> using bi-directional RNN archetecture. It is useful when we need to predict the feature that depends not only on previous info but also on the latter one -- depends on the whole seq.\n",
        "4. Difference between GRU and LSTM:\n",
        "<p> GRU has one less forget gate than LSTM, when we have unlimited amount of data and training resource, we should always go for LSTM.\n",
        "5. Large size of text input will make the RNN structure too deep and the gradients will explode or vanish."
      ]
    }
  ]
}