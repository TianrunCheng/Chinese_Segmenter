{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dp04train",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TianrunCheng/hello-world/blob/master/dp04train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ8GyGSP_IFE"
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPRcoRJ7LQeT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "d27c6bff-3e5b-4059-fda4-6f6ffa540dab"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.13.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.16.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.3->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.3->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.3->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQiBHpW13dQv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "dbda0722-324f-4368-80bb-a61ad8befbde"
      },
      "source": [
        "import os, sys, time, datetime, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "def cuda(arr):\n",
        "    if use_cuda:\n",
        "        return arr.cuda()\n",
        "    return arr\n",
        "\n",
        "\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer, WordpieceTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertForPreTraining, BertPreTrainedModel, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification\n",
        "# from pathlib import Path\n",
        "# import torch\n",
        "# import re\n",
        "from torch import Tensor\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "# from fastai.text import Tokenizer, Vocab\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "print('Total train num: ', len(train))\n",
        "train = train.dropna()\n",
        "print('Valid train num: ', len(train))\n",
        "\n",
        "train_textID = train.textID.tolist()\n",
        "train_text = train.text.tolist()\n",
        "train_selected_text = train.selected_text.tolist()\n",
        "train_sentiment = train.sentiment.tolist()\n",
        "\n",
        "print(train_text[:3])\n",
        "print(train_selected_text[:3])\n",
        "print(train_sentiment[:10])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Total train num:  27481\n",
            "Valid train num:  27480\n",
            "[' I`d have responded, if I were going', ' Sooo SAD I will miss you here in San Diego!!!', 'my boss is bullying me...']\n",
            "['I`d have responded, if I were going', 'Sooo SAD', 'bullying me']\n",
            "['neutral', 'negative', 'negative', 'negative', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3IMIPuDN18"
      },
      "source": [
        "\"\"\"\n",
        "Use Bert-tokenizer to tokenize train_text and train_selected_text\n",
        "Use Bert's dictionary to convert tokens to indexes\n",
        "Prepare training data to be ready to feed into BERT:\n",
        "\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])  #required\n",
        "    ### [batch_size, sequence_len], idx of input sequences\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    ### [batch_size, sequence_len] used when input sequence is smaller than max seq len in this batch\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    ### [batch_size, sequence_len], '0': sentence_A, '1': sentence_B\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# NUM of datapoints to use, max is 27480\n",
        "data_num = 27480\n",
        "validation_rate = 0.2 \n",
        "fixed_input_size = 200\n",
        "#############################\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "\n",
        "def text_to_ids(sentense):\n",
        "  tokens = tokenizer.tokenize(sentense)\n",
        "  tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  return tokens_ids\n",
        "\n",
        "#\n",
        "# padding each string to a default length, as in var: fixed_input_size\n",
        "# Also convert the format from list to np.darray\n",
        "def padding(s):\n",
        "  return np.array( s+ [0]*(fixed_input_size-len(s)) )\n",
        "\n",
        "input_idx = []\n",
        "input_mask = []\n",
        "output_idx = []\n",
        "for j in range( data_num ):\n",
        "  # if j<10:\n",
        "    # print(tokenizer.tokenize(\"[SEP]\"))\n",
        "  CLS_id = text_to_ids(\"[CLS]\")\n",
        "  sentiment_id = text_to_ids(train_sentiment[j])\n",
        "  SEP_id = text_to_ids(\"[SEP]\")\n",
        "  train_text_ids = text_to_ids(train_text[j])\n",
        "  # token([CLS] + sentiment_label + [SEP] + Train_text)\n",
        "  tokens_ids = CLS_id + sentiment_id + SEP_id + train_text_ids \n",
        "  input_idx.append(padding(tokens_ids))\n",
        "\n",
        "  # input_idx = [x, x, ..., x] 512\n",
        "  # real_input_idx = [x, ..., x] 200\n",
        "  #                  [sequence+pad]\n",
        "  # real_token_type = [0, 0, 0, 1,1,...,1] 200\n",
        "  # real_input_mask = [1,...,1,0,...,0] 200\n",
        "  #                   [sequence+pad]\n",
        "\n",
        "  mask = [1]* (len(train_text[j])+3)\n",
        "  input_mask.append(padding(mask))\n",
        "\n",
        "  \n",
        "  subset_tokens_ids = text_to_ids(train_selected_text[j])\n",
        "  whole_token_ids = text_to_ids(train_text[j])\n",
        "  l = len(subset_tokens_ids)\n",
        "  L = len(whole_token_ids)\n",
        "  output= [0]*2\n",
        "  j = 0\n",
        "  for j in range(L):\n",
        "    if whole_token_ids[j:j+l] == subset_tokens_ids:\n",
        "      # put 1 on all positions for the first occurence of the given substring\n",
        "      output+= [1]*l\n",
        "      break\n",
        "    else:\n",
        "      output+=[0] \n",
        "  output_idx.append(padding(output))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q41xENQJIgA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19026c1d-2320-4e4c-b547-e98faea678fa"
      },
      "source": [
        "\n",
        "######################################################## crucial constants\n",
        "#input length to be fed into BERT\n",
        "# fixed_input_size = 200\n",
        "\n",
        "# max 27840\n",
        "train_size = 24000 \n",
        "valid_size = 3000\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dropout_prob = 0.4\n",
        "hidden_size = 768\n",
        "########################\n",
        "\n",
        "nEpochs = 12\n",
        "learning_rate = 0.00002\n",
        "L2_lambda = 0.0\n",
        "\n",
        "############################################################\n",
        "\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(np.array(input_idx[:train_size])),\\\n",
        "              torch.from_numpy(np.array(input_mask[:train_size])),\\\n",
        "               torch.from_numpy(np.array(output_idx[:train_size])))\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size= batch_size)\n",
        "\n",
        "validation_data = TensorDataset(torch.from_numpy(np.array(input_idx[-valid_size:])),\\\n",
        "              torch.from_numpy(np.array(input_mask[-valid_size:])),\\\n",
        "               torch.from_numpy(np.array(output_idx[-valid_size:])),\\\n",
        "              torch.from_numpy(np.array( range(len(input_idx)-valid_size,len(input_idx)) ))\\\n",
        "              )\n",
        "validation_loader = DataLoader(validation_data, shuffle=True, batch_size= batch_size)\n",
        "\n",
        "\n",
        "\n",
        "class Model(BertPreTrainedModel):\n",
        "  def __init__(self,config):\n",
        "\n",
        "    super(Model,self).__init__(config)\n",
        "    self.bert = BertModel(config)\n",
        "    self.dropout = torch.nn.Dropout(dropout_prob)\n",
        "    self.ll = torch.nn.Linear(hidden_size, 2)\n",
        "    self.apply(self.init_bert_weights)\n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = True \n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None, labels=None ):\n",
        "    # token_type_ids = torch.zeros([len(input_ids), fixed_input_size], dtype=torch.long) # wrong input!\n",
        "    type_line = [1,1,1]\n",
        "    for i in range(fixed_input_size-3):\n",
        "      type_line.append(0)\n",
        "    token_type_ids = []\n",
        "    for j in range(len(input_ids)):\n",
        "      token_type_ids.append(type_line)\n",
        "    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long)\n",
        "    token_type_ids = cuda(token_type_ids)\n",
        "    output,pooled_output = self.bert(input_ids, token_type_ids=token_type_ids,\\\n",
        "                                     attention_mask=attention_mask, output_all_encoded_layers=False)\n",
        "    output = self.dropout(output)\n",
        "    output = self.ll(output)\n",
        "    return output\n",
        "  \n",
        "model = Model.from_pretrained(\"bert-base-uncased\")\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): BertLayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            "  (ll): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj48nsmVvcij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31751cba-276b-4fac-9a88-97e3741e5d80"
      },
      "source": [
        "\n",
        "def train_rnn_model_1(model, data_loader):\n",
        "\n",
        "    model = model.cuda()\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "\n",
        "    train_loss = 0\n",
        "    trained_sen = 0\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(nEpochs):\n",
        "        for idx, (input_idx, attention_mask, target) in enumerate(data_loader):\n",
        "            input_idx=torch.tensor(input_idx,dtype=torch.long)\n",
        "            attention_mask = torch.tensor(attention_mask,dtype=torch.long)\n",
        "            target=torch.tensor(target,dtype=torch.long)\n",
        "            input_idx, attention_mask, target = cuda(input_idx),cuda(attention_mask),cuda(target)\n",
        "            model.zero_grad()\n",
        "            out = model(input_idx, attention_mask)\n",
        "            \n",
        "            #loss = criterion(out, target)\n",
        "            loss = 0\n",
        "            for i in range(len(input_idx)):\n",
        "                loss += criterion(out[i][:200,:200], target[i][:200])\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.data.item()\n",
        "            trained_sen += len(input_idx)\n",
        "            \n",
        "            if idx % 5 == 0:\n",
        "                print(\"%2d %6d %8.3f\" % (epoch, idx*batch_size, train_loss/trained_sen))\n",
        "            \n",
        "            # torch.cuda.empty_cache()\n",
        "\n",
        "        # model_name = \"model\"+str(epoch)+'_'+ str(train_loss/trained_sen)+'.pth'\n",
        "        model_save_name = \"model\"+str(epoch)+'_'+ str(train_loss/trained_sen)+'.pth'\n",
        "        path = F\"/content/drive/My Drive/{model_save_name}\"  \n",
        "        torch.save(model.state_dict(), path)\n",
        "        # torch.save(model, model_name)\n",
        "                \n",
        "    torch.save(model, \"model_final.pth\")\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "train_rnn_model_1(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 0      0  153.688\n",
            " 0    160   52.290\n",
            " 0    320   36.385\n",
            " 0    480   30.335\n",
            " 0    640   26.630\n",
            " 0    800   24.146\n",
            " 0    960   22.516\n",
            " 0   1120   21.243\n",
            " 0   1280   20.253\n",
            " 0   1440   19.458\n",
            " 0   1600   18.921\n",
            " 0   1760   18.391\n",
            " 0   1920   17.944\n",
            " 0   2080   17.595\n",
            " 0   2240   17.301\n",
            " 0   2400   16.990\n",
            " 0   2560   16.771\n",
            " 0   2720   16.557\n",
            " 0   2880   16.388\n",
            " 0   3040   16.211\n",
            " 0   3200   16.024\n",
            " 0   3360   15.904\n",
            " 0   3520   15.768\n",
            " 0   3680   15.615\n",
            " 0   3840   15.454\n",
            " 0   4000   15.224\n",
            " 0   4160   14.938\n",
            " 0   4320   14.711\n",
            " 0   4480   14.482\n",
            " 0   4640   14.281\n",
            " 0   4800   14.070\n",
            " 0   4960   13.902\n",
            " 0   5120   13.710\n",
            " 0   5280   13.504\n",
            " 0   5440   13.356\n",
            " 0   5600   13.219\n",
            " 0   5760   13.069\n",
            " 0   5920   12.953\n",
            " 0   6080   12.793\n",
            " 0   6240   12.697\n",
            " 0   6400   12.594\n",
            " 0   6560   12.456\n",
            " 0   6720   12.331\n",
            " 0   6880   12.236\n",
            " 0   7040   12.139\n",
            " 0   7200   12.038\n",
            " 0   7360   11.935\n",
            " 0   7520   11.820\n",
            " 0   7680   11.722\n",
            " 0   7840   11.624\n",
            " 0   8000   11.536\n",
            " 0   8160   11.485\n",
            " 0   8320   11.403\n",
            " 0   8480   11.337\n",
            " 0   8640   11.282\n",
            " 0   8800   11.222\n",
            " 0   8960   11.149\n",
            " 0   9120   11.087\n",
            " 0   9280   11.025\n",
            " 0   9440   10.982\n",
            " 0   9600   10.927\n",
            " 0   9760   10.864\n",
            " 0   9920   10.801\n",
            " 0  10080   10.744\n",
            " 0  10240   10.684\n",
            " 0  10400   10.626\n",
            " 0  10560   10.592\n",
            " 0  10720   10.540\n",
            " 0  10880   10.490\n",
            " 0  11040   10.480\n",
            " 0  11200   10.431\n",
            " 0  11360   10.384\n",
            " 0  11520   10.336\n",
            " 0  11680   10.312\n",
            " 0  11840   10.288\n",
            " 0  12000   10.253\n",
            " 0  12160   10.207\n",
            " 0  12320   10.179\n",
            " 0  12480   10.138\n",
            " 0  12640   10.098\n",
            " 0  12800   10.050\n",
            " 0  12960   10.022\n",
            " 0  13120    9.972\n",
            " 0  13280    9.933\n",
            " 0  13440    9.914\n",
            " 0  13600    9.890\n",
            " 0  13760    9.860\n",
            " 0  13920    9.830\n",
            " 0  14080    9.805\n",
            " 0  14240    9.783\n",
            " 0  14400    9.755\n",
            " 0  14560    9.733\n",
            " 0  14720    9.709\n",
            " 0  14880    9.685\n",
            " 0  15040    9.663\n",
            " 0  15200    9.628\n",
            " 0  15360    9.601\n",
            " 0  15520    9.585\n",
            " 0  15680    9.573\n",
            " 0  15840    9.563\n",
            " 0  16000    9.533\n",
            " 0  16160    9.522\n",
            " 0  16320    9.502\n",
            " 0  16480    9.494\n",
            " 0  16640    9.483\n",
            " 0  16800    9.465\n",
            " 0  16960    9.440\n",
            " 0  17120    9.436\n",
            " 0  17280    9.414\n",
            " 0  17440    9.398\n",
            " 0  17600    9.371\n",
            " 0  17760    9.335\n",
            " 0  17920    9.320\n",
            " 0  18080    9.301\n",
            " 0  18240    9.282\n",
            " 0  18400    9.259\n",
            " 0  18560    9.234\n",
            " 0  18720    9.213\n",
            " 0  18880    9.199\n",
            " 0  19040    9.168\n",
            " 0  19200    9.156\n",
            " 0  19360    9.134\n",
            " 0  19520    9.113\n",
            " 0  19680    9.089\n",
            " 0  19840    9.087\n",
            " 0  20000    9.065\n",
            " 0  20160    9.064\n",
            " 0  20320    9.055\n",
            " 0  20480    9.048\n",
            " 0  20640    9.032\n",
            " 0  20800    9.015\n",
            " 0  20960    8.996\n",
            " 0  21120    8.984\n",
            " 0  21280    8.972\n",
            " 0  21440    8.950\n",
            " 0  21600    8.938\n",
            " 0  21760    8.928\n",
            " 0  21920    8.914\n",
            " 0  22080    8.911\n",
            " 0  22240    8.897\n",
            " 0  22400    8.892\n",
            " 0  22560    8.884\n",
            " 0  22720    8.870\n",
            " 0  22880    8.854\n",
            " 0  23040    8.832\n",
            " 0  23200    8.819\n",
            " 0  23360    8.805\n",
            " 0  23520    8.786\n",
            " 0  23680    8.769\n",
            " 0  23840    8.760\n",
            " 1      0    8.745\n",
            " 1    160    8.730\n",
            " 1    320    8.710\n",
            " 1    480    8.698\n",
            " 1    640    8.688\n",
            " 1    800    8.679\n",
            " 1    960    8.662\n",
            " 1   1120    8.650\n",
            " 1   1280    8.633\n",
            " 1   1440    8.619\n",
            " 1   1600    8.599\n",
            " 1   1760    8.590\n",
            " 1   1920    8.576\n",
            " 1   2080    8.566\n",
            " 1   2240    8.562\n",
            " 1   2400    8.551\n",
            " 1   2560    8.543\n",
            " 1   2720    8.529\n",
            " 1   2880    8.514\n",
            " 1   3040    8.496\n",
            " 1   3200    8.485\n",
            " 1   3360    8.475\n",
            " 1   3520    8.457\n",
            " 1   3680    8.444\n",
            " 1   3840    8.436\n",
            " 1   4000    8.420\n",
            " 1   4160    8.402\n",
            " 1   4320    8.399\n",
            " 1   4480    8.390\n",
            " 1   4640    8.383\n",
            " 1   4800    8.368\n",
            " 1   4960    8.362\n",
            " 1   5120    8.362\n",
            " 1   5280    8.351\n",
            " 1   5440    8.332\n",
            " 1   5600    8.323\n",
            " 1   5760    8.309\n",
            " 1   5920    8.297\n",
            " 1   6080    8.292\n",
            " 1   6240    8.278\n",
            " 1   6400    8.278\n",
            " 1   6560    8.271\n",
            " 1   6720    8.262\n",
            " 1   6880    8.244\n",
            " 1   7040    8.235\n",
            " 1   7200    8.221\n",
            " 1   7360    8.221\n",
            " 1   7520    8.222\n",
            " 1   7680    8.226\n",
            " 1   7840    8.213\n",
            " 1   8000    8.207\n",
            " 1   8160    8.198\n",
            " 1   8320    8.190\n",
            " 1   8480    8.185\n",
            " 1   8640    8.178\n",
            " 1   8800    8.172\n",
            " 1   8960    8.170\n",
            " 1   9120    8.161\n",
            " 1   9280    8.161\n",
            " 1   9440    8.150\n",
            " 1   9600    8.138\n",
            " 1   9760    8.137\n",
            " 1   9920    8.127\n",
            " 1  10080    8.118\n",
            " 1  10240    8.109\n",
            " 1  10400    8.103\n",
            " 1  10560    8.093\n",
            " 1  10720    8.083\n",
            " 1  10880    8.074\n",
            " 1  11040    8.065\n",
            " 1  11200    8.062\n",
            " 1  11360    8.058\n",
            " 1  11520    8.047\n",
            " 1  11680    8.043\n",
            " 1  11840    8.036\n",
            " 1  12000    8.028\n",
            " 1  12160    8.021\n",
            " 1  12320    8.018\n",
            " 1  12480    8.008\n",
            " 1  12640    7.998\n",
            " 1  12800    7.997\n",
            " 1  12960    7.996\n",
            " 1  13120    7.989\n",
            " 1  13280    7.980\n",
            " 1  13440    7.969\n",
            " 1  13600    7.958\n",
            " 1  13760    7.946\n",
            " 1  13920    7.943\n",
            " 1  14080    7.936\n",
            " 1  14240    7.922\n",
            " 1  14400    7.912\n",
            " 1  14560    7.913\n",
            " 1  14720    7.907\n",
            " 1  14880    7.901\n",
            " 1  15040    7.901\n",
            " 1  15200    7.899\n",
            " 1  15360    7.893\n",
            " 1  15520    7.891\n",
            " 1  15680    7.888\n",
            " 1  15840    7.886\n",
            " 1  16000    7.885\n",
            " 1  16160    7.880\n",
            " 1  16320    7.877\n",
            " 1  16480    7.875\n",
            " 1  16640    7.873\n",
            " 1  16800    7.867\n",
            " 1  16960    7.867\n",
            " 1  17120    7.863\n",
            " 1  17280    7.863\n",
            " 1  17440    7.857\n",
            " 1  17600    7.848\n",
            " 1  17760    7.844\n",
            " 1  17920    7.841\n",
            " 1  18080    7.839\n",
            " 1  18240    7.831\n",
            " 1  18400    7.828\n",
            " 1  18560    7.827\n",
            " 1  18720    7.823\n",
            " 1  18880    7.822\n",
            " 1  19040    7.818\n",
            " 1  19200    7.815\n",
            " 1  19360    7.807\n",
            " 1  19520    7.800\n",
            " 1  19680    7.796\n",
            " 1  19840    7.791\n",
            " 1  20000    7.789\n",
            " 1  20160    7.788\n",
            " 1  20320    7.782\n",
            " 1  20480    7.776\n",
            " 1  20640    7.775\n",
            " 1  20800    7.771\n",
            " 1  20960    7.765\n",
            " 1  21120    7.758\n",
            " 1  21280    7.752\n",
            " 1  21440    7.746\n",
            " 1  21600    7.743\n",
            " 1  21760    7.737\n",
            " 1  21920    7.733\n",
            " 1  22080    7.730\n",
            " 1  22240    7.726\n",
            " 1  22400    7.721\n",
            " 1  22560    7.721\n",
            " 1  22720    7.716\n",
            " 1  22880    7.715\n",
            " 1  23040    7.713\n",
            " 1  23200    7.709\n",
            " 1  23360    7.709\n",
            " 1  23520    7.702\n",
            " 1  23680    7.695\n",
            " 1  23840    7.695\n",
            " 2      0    7.692\n",
            " 2    160    7.687\n",
            " 2    320    7.685\n",
            " 2    480    7.682\n",
            " 2    640    7.674\n",
            " 2    800    7.669\n",
            " 2    960    7.661\n",
            " 2   1120    7.659\n",
            " 2   1280    7.653\n",
            " 2   1440    7.644\n",
            " 2   1600    7.637\n",
            " 2   1760    7.632\n",
            " 2   1920    7.624\n",
            " 2   2080    7.617\n",
            " 2   2240    7.608\n",
            " 2   2400    7.601\n",
            " 2   2560    7.595\n",
            " 2   2720    7.586\n",
            " 2   2880    7.583\n",
            " 2   3040    7.573\n",
            " 2   3200    7.569\n",
            " 2   3360    7.564\n",
            " 2   3520    7.559\n",
            " 2   3680    7.556\n",
            " 2   3840    7.552\n",
            " 2   4000    7.546\n",
            " 2   4160    7.542\n",
            " 2   4320    7.538\n",
            " 2   4480    7.535\n",
            " 2   4640    7.532\n",
            " 2   4800    7.526\n",
            " 2   4960    7.522\n",
            " 2   5120    7.518\n",
            " 2   5280    7.513\n",
            " 2   5440    7.507\n",
            " 2   5600    7.500\n",
            " 2   5760    7.492\n",
            " 2   5920    7.489\n",
            " 2   6080    7.487\n",
            " 2   6240    7.483\n",
            " 2   6400    7.480\n",
            " 2   6560    7.474\n",
            " 2   6720    7.469\n",
            " 2   6880    7.462\n",
            " 2   7040    7.458\n",
            " 2   7200    7.455\n",
            " 2   7360    7.450\n",
            " 2   7520    7.441\n",
            " 2   7680    7.436\n",
            " 2   7840    7.433\n",
            " 2   8000    7.427\n",
            " 2   8160    7.424\n",
            " 2   8320    7.425\n",
            " 2   8480    7.420\n",
            " 2   8640    7.414\n",
            " 2   8800    7.408\n",
            " 2   8960    7.404\n",
            " 2   9120    7.401\n",
            " 2   9280    7.397\n",
            " 2   9440    7.393\n",
            " 2   9600    7.389\n",
            " 2   9760    7.386\n",
            " 2   9920    7.386\n",
            " 2  10080    7.386\n",
            " 2  10240    7.381\n",
            " 2  10400    7.377\n",
            " 2  10560    7.380\n",
            " 2  10720    7.379\n",
            " 2  10880    7.376\n",
            " 2  11040    7.374\n",
            " 2  11200    7.369\n",
            " 2  11360    7.367\n",
            " 2  11520    7.362\n",
            " 2  11680    7.355\n",
            " 2  11840    7.350\n",
            " 2  12000    7.347\n",
            " 2  12160    7.342\n",
            " 2  12320    7.339\n",
            " 2  12480    7.332\n",
            " 2  12640    7.329\n",
            " 2  12800    7.325\n",
            " 2  12960    7.320\n",
            " 2  13120    7.317\n",
            " 2  13280    7.315\n",
            " 2  13440    7.312\n",
            " 2  13600    7.309\n",
            " 2  13760    7.304\n",
            " 2  13920    7.303\n",
            " 2  14080    7.299\n",
            " 2  14240    7.297\n",
            " 2  14400    7.293\n",
            " 2  14560    7.288\n",
            " 2  14720    7.282\n",
            " 2  14880    7.281\n",
            " 2  15040    7.277\n",
            " 2  15200    7.276\n",
            " 2  15360    7.273\n",
            " 2  15520    7.268\n",
            " 2  15680    7.275\n",
            " 2  15840    7.273\n",
            " 2  16000    7.270\n",
            " 2  16160    7.267\n",
            " 2  16320    7.267\n",
            " 2  16480    7.264\n",
            " 2  16640    7.263\n",
            " 2  16800    7.260\n",
            " 2  16960    7.259\n",
            " 2  17120    7.256\n",
            " 2  17280    7.253\n",
            " 2  17440    7.252\n",
            " 2  17600    7.248\n",
            " 2  17760    7.244\n",
            " 2  17920    7.240\n",
            " 2  18080    7.234\n",
            " 2  18240    7.232\n",
            " 2  18400    7.230\n",
            " 2  18560    7.225\n",
            " 2  18720    7.223\n",
            " 2  18880    7.223\n",
            " 2  19040    7.221\n",
            " 2  19200    7.219\n",
            " 2  19360    7.214\n",
            " 2  19520    7.209\n",
            " 2  19680    7.207\n",
            " 2  19840    7.203\n",
            " 2  20000    7.200\n",
            " 2  20160    7.199\n",
            " 2  20320    7.196\n",
            " 2  20480    7.191\n",
            " 2  20640    7.185\n",
            " 2  20800    7.183\n",
            " 2  20960    7.179\n",
            " 2  21120    7.179\n",
            " 2  21280    7.178\n",
            " 2  21440    7.175\n",
            " 2  21600    7.172\n",
            " 2  21760    7.168\n",
            " 2  21920    7.169\n",
            " 2  22080    7.170\n",
            " 2  22240    7.166\n",
            " 2  22400    7.165\n",
            " 2  22560    7.160\n",
            " 2  22720    7.160\n",
            " 2  22880    7.160\n",
            " 2  23040    7.161\n",
            " 2  23200    7.158\n",
            " 2  23360    7.157\n",
            " 2  23520    7.155\n",
            " 2  23680    7.153\n",
            " 2  23840    7.151\n",
            " 3      0    7.148\n",
            " 3    160    7.144\n",
            " 3    320    7.139\n",
            " 3    480    7.135\n",
            " 3    640    7.130\n",
            " 3    800    7.125\n",
            " 3    960    7.118\n",
            " 3   1120    7.113\n",
            " 3   1280    7.109\n",
            " 3   1440    7.105\n",
            " 3   1600    7.101\n",
            " 3   1760    7.096\n",
            " 3   1920    7.090\n",
            " 3   2080    7.085\n",
            " 3   2240    7.081\n",
            " 3   2400    7.078\n",
            " 3   2560    7.075\n",
            " 3   2720    7.070\n",
            " 3   2880    7.066\n",
            " 3   3040    7.062\n",
            " 3   3200    7.059\n",
            " 3   3360    7.053\n",
            " 3   3520    7.049\n",
            " 3   3680    7.044\n",
            " 3   3840    7.042\n",
            " 3   4000    7.037\n",
            " 3   4160    7.035\n",
            " 3   4320    7.029\n",
            " 3   4480    7.028\n",
            " 3   4640    7.024\n",
            " 3   4800    7.022\n",
            " 3   4960    7.019\n",
            " 3   5120    7.013\n",
            " 3   5280    7.008\n",
            " 3   5440    7.005\n",
            " 3   5600    7.001\n",
            " 3   5760    6.996\n",
            " 3   5920    6.991\n",
            " 3   6080    6.986\n",
            " 3   6240    6.984\n",
            " 3   6400    6.980\n",
            " 3   6560    6.977\n",
            " 3   6720    6.973\n",
            " 3   6880    6.970\n",
            " 3   7040    6.966\n",
            " 3   7200    6.963\n",
            " 3   7360    6.959\n",
            " 3   7520    6.953\n",
            " 3   7680    6.951\n",
            " 3   7840    6.948\n",
            " 3   8000    6.943\n",
            " 3   8160    6.938\n",
            " 3   8320    6.934\n",
            " 3   8480    6.932\n",
            " 3   8640    6.928\n",
            " 3   8800    6.927\n",
            " 3   8960    6.923\n",
            " 3   9120    6.921\n",
            " 3   9280    6.918\n",
            " 3   9440    6.914\n",
            " 3   9600    6.911\n",
            " 3   9760    6.906\n",
            " 3   9920    6.901\n",
            " 3  10080    6.898\n",
            " 3  10240    6.892\n",
            " 3  10400    6.890\n",
            " 3  10560    6.887\n",
            " 3  10720    6.885\n",
            " 3  10880    6.882\n",
            " 3  11040    6.878\n",
            " 3  11200    6.877\n",
            " 3  11360    6.872\n",
            " 3  11520    6.872\n",
            " 3  11680    6.867\n",
            " 3  11840    6.866\n",
            " 3  12000    6.865\n",
            " 3  12160    6.861\n",
            " 3  12320    6.859\n",
            " 3  12480    6.854\n",
            " 3  12640    6.851\n",
            " 3  12800    6.849\n",
            " 3  12960    6.847\n",
            " 3  13120    6.844\n",
            " 3  13280    6.843\n",
            " 3  13440    6.839\n",
            " 3  13600    6.837\n",
            " 3  13760    6.834\n",
            " 3  13920    6.829\n",
            " 3  14080    6.827\n",
            " 3  14240    6.823\n",
            " 3  14400    6.821\n",
            " 3  14560    6.820\n",
            " 3  14720    6.817\n",
            " 3  14880    6.814\n",
            " 3  15040    6.812\n",
            " 3  15200    6.810\n",
            " 3  15360    6.808\n",
            " 3  15520    6.805\n",
            " 3  15680    6.801\n",
            " 3  15840    6.799\n",
            " 3  16000    6.794\n",
            " 3  16160    6.792\n",
            " 3  16320    6.790\n",
            " 3  16480    6.788\n",
            " 3  16640    6.788\n",
            " 3  16800    6.787\n",
            " 3  16960    6.784\n",
            " 3  17120    6.782\n",
            " 3  17280    6.781\n",
            " 3  17440    6.778\n",
            " 3  17600    6.776\n",
            " 3  17760    6.772\n",
            " 3  17920    6.769\n",
            " 3  18080    6.766\n",
            " 3  18240    6.764\n",
            " 3  18400    6.760\n",
            " 3  18560    6.756\n",
            " 3  18720    6.752\n",
            " 3  18880    6.749\n",
            " 3  19040    6.744\n",
            " 3  19200    6.742\n",
            " 3  19360    6.738\n",
            " 3  19520    6.736\n",
            " 3  19680    6.734\n",
            " 3  19840    6.733\n",
            " 3  20000    6.730\n",
            " 3  20160    6.729\n",
            " 3  20320    6.727\n",
            " 3  20480    6.727\n",
            " 3  20640    6.726\n",
            " 3  20800    6.722\n",
            " 3  20960    6.719\n",
            " 3  21120    6.717\n",
            " 3  21280    6.713\n",
            " 3  21440    6.710\n",
            " 3  21600    6.707\n",
            " 3  21760    6.703\n",
            " 3  21920    6.701\n",
            " 3  22080    6.698\n",
            " 3  22240    6.695\n",
            " 3  22400    6.693\n",
            " 3  22560    6.691\n",
            " 3  22720    6.688\n",
            " 3  22880    6.688\n",
            " 3  23040    6.685\n",
            " 3  23200    6.682\n",
            " 3  23360    6.681\n",
            " 3  23520    6.678\n",
            " 3  23680    6.676\n",
            " 3  23840    6.674\n",
            " 4      0    6.672\n",
            " 4    160    6.667\n",
            " 4    320    6.661\n",
            " 4    480    6.659\n",
            " 4    640    6.654\n",
            " 4    800    6.650\n",
            " 4    960    6.646\n",
            " 4   1120    6.641\n",
            " 4   1280    6.637\n",
            " 4   1440    6.633\n",
            " 4   1600    6.627\n",
            " 4   1760    6.621\n",
            " 4   1920    6.618\n",
            " 4   2080    6.613\n",
            " 4   2240    6.609\n",
            " 4   2400    6.606\n",
            " 4   2560    6.601\n",
            " 4   2720    6.595\n",
            " 4   2880    6.592\n",
            " 4   3040    6.587\n",
            " 4   3200    6.584\n",
            " 4   3360    6.579\n",
            " 4   3520    6.574\n",
            " 4   3680    6.570\n",
            " 4   3840    6.565\n",
            " 4   4000    6.561\n",
            " 4   4160    6.559\n",
            " 4   4320    6.554\n",
            " 4   4480    6.551\n",
            " 4   4640    6.547\n",
            " 4   4800    6.543\n",
            " 4   4960    6.539\n",
            " 4   5120    6.535\n",
            " 4   5280    6.533\n",
            " 4   5440    6.529\n",
            " 4   5600    6.525\n",
            " 4   5760    6.521\n",
            " 4   5920    6.517\n",
            " 4   6080    6.513\n",
            " 4   6240    6.509\n",
            " 4   6400    6.505\n",
            " 4   6560    6.502\n",
            " 4   6720    6.497\n",
            " 4   6880    6.493\n",
            " 4   7040    6.490\n",
            " 4   7200    6.486\n",
            " 4   7360    6.483\n",
            " 4   7520    6.478\n",
            " 4   7680    6.474\n",
            " 4   7840    6.469\n",
            " 4   8000    6.465\n",
            " 4   8160    6.462\n",
            " 4   8320    6.459\n",
            " 4   8480    6.455\n",
            " 4   8640    6.452\n",
            " 4   8800    6.448\n",
            " 4   8960    6.444\n",
            " 4   9120    6.439\n",
            " 4   9280    6.436\n",
            " 4   9440    6.432\n",
            " 4   9600    6.428\n",
            " 4   9760    6.423\n",
            " 4   9920    6.418\n",
            " 4  10080    6.414\n",
            " 4  10240    6.412\n",
            " 4  10400    6.408\n",
            " 4  10560    6.404\n",
            " 4  10720    6.403\n",
            " 4  10880    6.399\n",
            " 4  11040    6.395\n",
            " 4  11200    6.391\n",
            " 4  11360    6.387\n",
            " 4  11520    6.384\n",
            " 4  11680    6.383\n",
            " 4  11840    6.379\n",
            " 4  12000    6.376\n",
            " 4  12160    6.373\n",
            " 4  12320    6.369\n",
            " 4  12480    6.366\n",
            " 4  12640    6.362\n",
            " 4  12800    6.359\n",
            " 4  12960    6.356\n",
            " 4  13120    6.352\n",
            " 4  13280    6.349\n",
            " 4  13440    6.345\n",
            " 4  13600    6.341\n",
            " 4  13760    6.338\n",
            " 4  13920    6.334\n",
            " 4  14080    6.331\n",
            " 4  14240    6.329\n",
            " 4  14400    6.325\n",
            " 4  14560    6.320\n",
            " 4  14720    6.317\n",
            " 4  14880    6.314\n",
            " 4  15040    6.310\n",
            " 4  15200    6.307\n",
            " 4  15360    6.302\n",
            " 4  15520    6.300\n",
            " 4  15680    6.297\n",
            " 4  15840    6.294\n",
            " 4  16000    6.291\n",
            " 4  16160    6.289\n",
            " 4  16320    6.287\n",
            " 4  16480    6.286\n",
            " 4  16640    6.281\n",
            " 4  16800    6.277\n",
            " 4  16960    6.275\n",
            " 4  17120    6.274\n",
            " 4  17280    6.270\n",
            " 4  17440    6.268\n",
            " 4  17600    6.266\n",
            " 4  17760    6.264\n",
            " 4  17920    6.262\n",
            " 4  18080    6.259\n",
            " 4  18240    6.256\n",
            " 4  18400    6.254\n",
            " 4  18560    6.250\n",
            " 4  18720    6.247\n",
            " 4  18880    6.243\n",
            " 4  19040    6.240\n",
            " 4  19200    6.237\n",
            " 4  19360    6.234\n",
            " 4  19520    6.232\n",
            " 4  19680    6.230\n",
            " 4  19840    6.226\n",
            " 4  20000    6.223\n",
            " 4  20160    6.219\n",
            " 4  20320    6.215\n",
            " 4  20480    6.211\n",
            " 4  20640    6.208\n",
            " 4  20800    6.204\n",
            " 4  20960    6.201\n",
            " 4  21120    6.198\n",
            " 4  21280    6.195\n",
            " 4  21440    6.191\n",
            " 4  21600    6.188\n",
            " 4  21760    6.186\n",
            " 4  21920    6.183\n",
            " 4  22080    6.180\n",
            " 4  22240    6.176\n",
            " 4  22400    6.173\n",
            " 4  22560    6.171\n",
            " 4  22720    6.169\n",
            " 4  22880    6.166\n",
            " 4  23040    6.164\n",
            " 4  23200    6.160\n",
            " 4  23360    6.157\n",
            " 4  23520    6.154\n",
            " 4  23680    6.151\n",
            " 4  23840    6.149\n",
            " 5      0    6.145\n",
            " 5    160    6.142\n",
            " 5    320    6.139\n",
            " 5    480    6.135\n",
            " 5    640    6.132\n",
            " 5    800    6.128\n",
            " 5    960    6.123\n",
            " 5   1120    6.119\n",
            " 5   1280    6.115\n",
            " 5   1440    6.111\n",
            " 5   1600    6.106\n",
            " 5   1760    6.102\n",
            " 5   1920    6.097\n",
            " 5   2080    6.092\n",
            " 5   2240    6.088\n",
            " 5   2400    6.083\n",
            " 5   2560    6.079\n",
            " 5   2720    6.075\n",
            " 5   2880    6.071\n",
            " 5   3040    6.067\n",
            " 5   3200    6.064\n",
            " 5   3360    6.060\n",
            " 5   3520    6.056\n",
            " 5   3680    6.052\n",
            " 5   3840    6.048\n",
            " 5   4000    6.043\n",
            " 5   4160    6.039\n",
            " 5   4320    6.035\n",
            " 5   4480    6.031\n",
            " 5   4640    6.027\n",
            " 5   4800    6.024\n",
            " 5   4960    6.020\n",
            " 5   5120    6.017\n",
            " 5   5280    6.013\n",
            " 5   5440    6.009\n",
            " 5   5600    6.006\n",
            " 5   5760    6.002\n",
            " 5   5920    5.997\n",
            " 5   6080    5.993\n",
            " 5   6240    5.989\n",
            " 5   6400    5.985\n",
            " 5   6560    5.981\n",
            " 5   6720    5.976\n",
            " 5   6880    5.972\n",
            " 5   7040    5.968\n",
            " 5   7200    5.965\n",
            " 5   7360    5.962\n",
            " 5   7520    5.959\n",
            " 5   7680    5.955\n",
            " 5   7840    5.952\n",
            " 5   8000    5.948\n",
            " 5   8160    5.944\n",
            " 5   8320    5.940\n",
            " 5   8480    5.937\n",
            " 5   8640    5.934\n",
            " 5   8800    5.930\n",
            " 5   8960    5.926\n",
            " 5   9120    5.922\n",
            " 5   9280    5.919\n",
            " 5   9440    5.915\n",
            " 5   9600    5.912\n",
            " 5   9760    5.908\n",
            " 5   9920    5.905\n",
            " 5  10080    5.903\n",
            " 5  10240    5.900\n",
            " 5  10400    5.897\n",
            " 5  10560    5.893\n",
            " 5  10720    5.889\n",
            " 5  10880    5.886\n",
            " 5  11040    5.882\n",
            " 5  11200    5.878\n",
            " 5  11360    5.875\n",
            " 5  11520    5.873\n",
            " 5  11680    5.869\n",
            " 5  11840    5.865\n",
            " 5  12000    5.862\n",
            " 5  12160    5.860\n",
            " 5  12320    5.857\n",
            " 5  12480    5.854\n",
            " 5  12640    5.851\n",
            " 5  12800    5.847\n",
            " 5  12960    5.843\n",
            " 5  13120    5.840\n",
            " 5  13280    5.836\n",
            " 5  13440    5.834\n",
            " 5  13600    5.832\n",
            " 5  13760    5.827\n",
            " 5  13920    5.824\n",
            " 5  14080    5.821\n",
            " 5  14240    5.818\n",
            " 5  14400    5.815\n",
            " 5  14560    5.812\n",
            " 5  14720    5.808\n",
            " 5  14880    5.805\n",
            " 5  15040    5.801\n",
            " 5  15200    5.797\n",
            " 5  15360    5.794\n",
            " 5  15520    5.791\n",
            " 5  15680    5.789\n",
            " 5  15840    5.787\n",
            " 5  16000    5.784\n",
            " 5  16160    5.780\n",
            " 5  16320    5.778\n",
            " 5  16480    5.774\n",
            " 5  16640    5.772\n",
            " 5  16800    5.768\n",
            " 5  16960    5.766\n",
            " 5  17120    5.763\n",
            " 5  17280    5.760\n",
            " 5  17440    5.756\n",
            " 5  17600    5.754\n",
            " 5  17760    5.751\n",
            " 5  17920    5.748\n",
            " 5  18080    5.745\n",
            " 5  18240    5.742\n",
            " 5  18400    5.739\n",
            " 5  18560    5.735\n",
            " 5  18720    5.732\n",
            " 5  18880    5.729\n",
            " 5  19040    5.725\n",
            " 5  19200    5.722\n",
            " 5  19360    5.720\n",
            " 5  19520    5.717\n",
            " 5  19680    5.714\n",
            " 5  19840    5.712\n",
            " 5  20000    5.709\n",
            " 5  20160    5.706\n",
            " 5  20320    5.703\n",
            " 5  20480    5.700\n",
            " 5  20640    5.698\n",
            " 5  20800    5.695\n",
            " 5  20960    5.692\n",
            " 5  21120    5.689\n",
            " 5  21280    5.686\n",
            " 5  21440    5.683\n",
            " 5  21600    5.680\n",
            " 5  21760    5.677\n",
            " 5  21920    5.676\n",
            " 5  22080    5.673\n",
            " 5  22240    5.670\n",
            " 5  22400    5.667\n",
            " 5  22560    5.664\n",
            " 5  22720    5.661\n",
            " 5  22880    5.659\n",
            " 5  23040    5.656\n",
            " 5  23200    5.653\n",
            " 5  23360    5.652\n",
            " 5  23520    5.649\n",
            " 5  23680    5.647\n",
            " 5  23840    5.643\n",
            " 6      0    5.640\n",
            " 6    160    5.637\n",
            " 6    320    5.633\n",
            " 6    480    5.630\n",
            " 6    640    5.627\n",
            " 6    800    5.624\n",
            " 6    960    5.620\n",
            " 6   1120    5.617\n",
            " 6   1280    5.613\n",
            " 6   1440    5.610\n",
            " 6   1600    5.606\n",
            " 6   1760    5.602\n",
            " 6   1920    5.599\n",
            " 6   2080    5.596\n",
            " 6   2240    5.592\n",
            " 6   2400    5.588\n",
            " 6   2560    5.584\n",
            " 6   2720    5.581\n",
            " 6   2880    5.577\n",
            " 6   3040    5.574\n",
            " 6   3200    5.571\n",
            " 6   3360    5.569\n",
            " 6   3520    5.566\n",
            " 6   3680    5.562\n",
            " 6   3840    5.559\n",
            " 6   4000    5.556\n",
            " 6   4160    5.553\n",
            " 6   4320    5.550\n",
            " 6   4480    5.547\n",
            " 6   4640    5.544\n",
            " 6   4800    5.540\n",
            " 6   4960    5.537\n",
            " 6   5120    5.533\n",
            " 6   5280    5.530\n",
            " 6   5440    5.526\n",
            " 6   5600    5.523\n",
            " 6   5760    5.520\n",
            " 6   5920    5.517\n",
            " 6   6080    5.514\n",
            " 6   6240    5.509\n",
            " 6   6400    5.506\n",
            " 6   6560    5.503\n",
            " 6   6720    5.500\n",
            " 6   6880    5.497\n",
            " 6   7040    5.494\n",
            " 6   7200    5.490\n",
            " 6   7360    5.487\n",
            " 6   7520    5.484\n",
            " 6   7680    5.481\n",
            " 6   7840    5.478\n",
            " 6   8000    5.474\n",
            " 6   8160    5.471\n",
            " 6   8320    5.467\n",
            " 6   8480    5.464\n",
            " 6   8640    5.460\n",
            " 6   8800    5.457\n",
            " 6   8960    5.453\n",
            " 6   9120    5.450\n",
            " 6   9280    5.447\n",
            " 6   9440    5.444\n",
            " 6   9600    5.440\n",
            " 6   9760    5.437\n",
            " 6   9920    5.434\n",
            " 6  10080    5.431\n",
            " 6  10240    5.428\n",
            " 6  10400    5.425\n",
            " 6  10560    5.422\n",
            " 6  10720    5.419\n",
            " 6  10880    5.416\n",
            " 6  11040    5.413\n",
            " 6  11200    5.410\n",
            " 6  11360    5.407\n",
            " 6  11520    5.404\n",
            " 6  11680    5.402\n",
            " 6  11840    5.399\n",
            " 6  12000    5.396\n",
            " 6  12160    5.392\n",
            " 6  12320    5.390\n",
            " 6  12480    5.387\n",
            " 6  12640    5.385\n",
            " 6  12800    5.382\n",
            " 6  12960    5.380\n",
            " 6  13120    5.377\n",
            " 6  13280    5.375\n",
            " 6  13440    5.372\n",
            " 6  13600    5.369\n",
            " 6  13760    5.366\n",
            " 6  13920    5.363\n",
            " 6  14080    5.359\n",
            " 6  14240    5.356\n",
            " 6  14400    5.353\n",
            " 6  14560    5.351\n",
            " 6  14720    5.348\n",
            " 6  14880    5.346\n",
            " 6  15040    5.343\n",
            " 6  15200    5.341\n",
            " 6  15360    5.339\n",
            " 6  15520    5.336\n",
            " 6  15680    5.333\n",
            " 6  15840    5.330\n",
            " 6  16000    5.327\n",
            " 6  16160    5.324\n",
            " 6  16320    5.321\n",
            " 6  16480    5.318\n",
            " 6  16640    5.315\n",
            " 6  16800    5.312\n",
            " 6  16960    5.309\n",
            " 6  17120    5.307\n",
            " 6  17280    5.304\n",
            " 6  17440    5.301\n",
            " 6  17600    5.298\n",
            " 6  17760    5.296\n",
            " 6  17920    5.293\n",
            " 6  18080    5.290\n",
            " 6  18240    5.287\n",
            " 6  18400    5.284\n",
            " 6  18560    5.282\n",
            " 6  18720    5.279\n",
            " 6  18880    5.276\n",
            " 6  19040    5.274\n",
            " 6  19200    5.271\n",
            " 6  19360    5.269\n",
            " 6  19520    5.266\n",
            " 6  19680    5.263\n",
            " 6  19840    5.261\n",
            " 6  20000    5.259\n",
            " 6  20160    5.256\n",
            " 6  20320    5.253\n",
            " 6  20480    5.250\n",
            " 6  20640    5.247\n",
            " 6  20800    5.244\n",
            " 6  20960    5.242\n",
            " 6  21120    5.241\n",
            " 6  21280    5.238\n",
            " 6  21440    5.235\n",
            " 6  21600    5.231\n",
            " 6  21760    5.229\n",
            " 6  21920    5.227\n",
            " 6  22080    5.224\n",
            " 6  22240    5.221\n",
            " 6  22400    5.219\n",
            " 6  22560    5.216\n",
            " 6  22720    5.213\n",
            " 6  22880    5.211\n",
            " 6  23040    5.208\n",
            " 6  23200    5.206\n",
            " 6  23360    5.203\n",
            " 6  23520    5.201\n",
            " 6  23680    5.198\n",
            " 6  23840    5.196\n",
            " 7      0    5.193\n",
            " 7    160    5.190\n",
            " 7    320    5.187\n",
            " 7    480    5.184\n",
            " 7    640    5.182\n",
            " 7    800    5.179\n",
            " 7    960    5.176\n",
            " 7   1120    5.173\n",
            " 7   1280    5.171\n",
            " 7   1440    5.168\n",
            " 7   1600    5.165\n",
            " 7   1760    5.162\n",
            " 7   1920    5.160\n",
            " 7   2080    5.156\n",
            " 7   2240    5.153\n",
            " 7   2400    5.150\n",
            " 7   2560    5.148\n",
            " 7   2720    5.145\n",
            " 7   2880    5.142\n",
            " 7   3040    5.139\n",
            " 7   3200    5.136\n",
            " 7   3360    5.133\n",
            " 7   3520    5.130\n",
            " 7   3680    5.127\n",
            " 7   3840    5.124\n",
            " 7   4000    5.120\n",
            " 7   4160    5.118\n",
            " 7   4320    5.115\n",
            " 7   4480    5.112\n",
            " 7   4640    5.109\n",
            " 7   4800    5.106\n",
            " 7   4960    5.103\n",
            " 7   5120    5.100\n",
            " 7   5280    5.097\n",
            " 7   5440    5.095\n",
            " 7   5600    5.092\n",
            " 7   5760    5.089\n",
            " 7   5920    5.086\n",
            " 7   6080    5.083\n",
            " 7   6240    5.080\n",
            " 7   6400    5.078\n",
            " 7   6560    5.075\n",
            " 7   6720    5.072\n",
            " 7   6880    5.069\n",
            " 7   7040    5.067\n",
            " 7   7200    5.063\n",
            " 7   7360    5.061\n",
            " 7   7520    5.058\n",
            " 7   7680    5.055\n",
            " 7   7840    5.052\n",
            " 7   8000    5.049\n",
            " 7   8160    5.047\n",
            " 7   8320    5.044\n",
            " 7   8480    5.041\n",
            " 7   8640    5.038\n",
            " 7   8800    5.036\n",
            " 7   8960    5.033\n",
            " 7   9120    5.030\n",
            " 7   9280    5.027\n",
            " 7   9440    5.024\n",
            " 7   9600    5.022\n",
            " 7   9760    5.020\n",
            " 7   9920    5.017\n",
            " 7  10080    5.014\n",
            " 7  10240    5.011\n",
            " 7  10400    5.008\n",
            " 7  10560    5.006\n",
            " 7  10720    5.004\n",
            " 7  10880    5.002\n",
            " 7  11040    4.999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f42cc8895360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtrain_rnn_model_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-f42cc8895360>\u001b[0m in \u001b[0;36mtrain_rnn_model_1\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-y5nqFn5yR2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afead21a-a4fa-4611-c3bf-49dfd0eb58e9"
      },
      "source": [
        "import torch.nn as nn\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/model6_5.193322490601313.pth\"))\n",
        "# model = torch.load(\"/content/drive/My Drive/model6_5.138287643682389.pth\")\n",
        "model.eval()\n",
        "def jaccard(list1, list2):\n",
        "    a = set(list1)\n",
        "    b = set(list2)\n",
        "    c = a.intersection(b)\n",
        "    if (len(a) + len(b) - len(c))==0:\n",
        "        print('--------error:',len(a), len(b), len(c))\n",
        "        return 0\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "def getWord(val_ls):\n",
        "    rst = []\n",
        "    for val in val_ls:\n",
        "        if val <= 2:\n",
        "            continue\n",
        "        ky = list(vocab_to_int.keys())[list(vocab_to_int.values()).index(val)]\n",
        "        rst.append(ky)\n",
        "    return rst\n",
        "\n",
        "def getTarget(val_ls, label_ls):\n",
        "    rst = []\n",
        "    for i in range(len(val_ls)):\n",
        "        if label_ls[i] == 1:\n",
        "            rst.append(val_ls[i])\n",
        "    return rst\n",
        "\n",
        "def out_to_class(out):\n",
        "    m = nn.Softmax(2)\n",
        "    a = m(out).cpu()\n",
        "    a= a.detach().numpy()\n",
        "    return np.argmax(a,axis=-1)\n",
        "\n",
        "def ids_to_string(ids):\n",
        "  output_substring = \" \".join( tokenizer.convert_ids_to_tokens(ids))\n",
        "  fine_output_substring = output_substring.replace(' ##', '')\n",
        "  return fine_output_substring\n",
        "def boolean_to_string(answer_boolean,orig_ids):\n",
        "   ans_ids  = []\n",
        "   for j,out in enumerate(answer_boolean):\n",
        "    if out ==1:\n",
        "      ans_ids.append(orig_ids[j+1])\n",
        "   return ids_to_string(ans_ids)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  score = 0.0\n",
        "  check = True\n",
        "  for idx, (orin_input_idx, attention_mask, output_boolean, indices) in enumerate(validation_loader):\n",
        "\n",
        "      input_idx=torch.tensor(orin_input_idx,dtype=torch.long)\n",
        "      attention_mask = torch.tensor(attention_mask,dtype=torch.long)\n",
        "      input_idx, attention_mask = cuda(input_idx),cuda(attention_mask)\n",
        "      out = model(input_idx, attention_mask)\n",
        "      answers = out_to_class(out)\n",
        "      \n",
        "      for i,answer in enumerate(answers):\n",
        "        orig_ids = orin_input_idx[i].numpy()\n",
        "        output = boolean_to_string(answer, orig_ids)\n",
        "        score += jaccard(output, train_selected_text[indices[i]])\n",
        "\n",
        "        if check:\n",
        "          print(ids_to_string(orin_input_idx[i].numpy()))\n",
        "          print(\"######Our output##########\")\n",
        "          print(boolean_to_string(answer, orig_ids))\n",
        "          print(\"#####Target######\")\n",
        "          print(train_selected_text[indices[i]])\n",
        "          #print(boolean_to_string(output_boolean[i],orig_ids))\n",
        "          print(\"#######END#######\")\n",
        "      check = False \n",
        "      #break \n",
        "  print(score)\n",
        "  print(score/float(valid_size))\n",
        "\n",
        "  # print(err_count)\n",
        "  # print(total_count, 1 - err_count/total_count)\n",
        "  # # print(score/len(test_x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CLS] positive [SEP] courtney seems like a cool chick & shes pretty [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "seems like a cool chick & shes pretty\n",
            "#####Target######\n",
            "pretty\n",
            "#######END#######\n",
            "[CLS] positive [SEP] heading out to dinner in a minute - cant wait to have food . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "heading - cant wait to have food .\n",
            "#####Target######\n",
            "cant wait to have food.\n",
            "#######END#######\n",
            "[CLS] positive [SEP] oh top gear ( uk ) , how i love thee [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "how i love thee\n",
            "#####Target######\n",
            "how I love thee\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] off to school . who knows what to expect ? bring on life and all of it ` s suprises ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "off to school . who knows what to expect ? bring on life and all of it ` s suprises !\n",
            "#####Target######\n",
            "Off to school. Who knows what to expect? Bring on life and all of it`s suprises!\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] _ i guess we will wait & see . . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "_ i guess we will wait & see . .\n",
            "#####Target######\n",
            "_ i guess we will wait & see..\n",
            "#######END#######\n",
            "[CLS] positive [SEP] wow . that ` s looks really good . i wish i had some . was it good ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "wow . that ` s looks really good .\n",
            "#####Target######\n",
            "That`s looks really good.\n",
            "#######END#######\n",
            "[CLS] positive [SEP] happy mothers day . take ur mothers into a special place . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "happy mothers day .\n",
            "#####Target######\n",
            "Happy\n",
            "#######END#######\n",
            "[CLS] positive [SEP] goodbye picnic for a classmate today but the weather was * * * * beautifuul ! ! ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "* * * * beautifuul ! ! !\n",
            "#####Target######\n",
            "**** beautifuul!\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] types too fast for her own good . and has been craving pizza for 2 + wks . gah i hate not having a job i want pizza ! lmaoz why me ? ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "gah i hate not having a job i want pizza ! l\n",
            "#####Target######\n",
            "types too fast for her own good. and has been craving pizza for 2+ wks. GAH i hate not having a job i want pizza!  lmaoz why me?!\n",
            "#######END#######\n",
            "[CLS] negative [SEP] steve makes fruit smoothies for me each day & they are berry delicious , i made mine today & it was berry , berry bad [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "they berry delicious , bad\n",
            "#####Target######\n",
            "bad\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] i really need a job . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "i really need a job .\n",
            "#####Target######\n",
            "I really need a job.\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] theres a fricken prisnor trans outside r hotel ! 3 / 10 so far [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "theres a fricken prisnor trans outside r hotel ! 3 / 10 so far\n",
            "#####Target######\n",
            "Theres a fricken prisnor trans outside r hotel!  3/10 so far\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] you need to play something from mary poppins at them , everyone knows that ` s how rooms get tidy [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "you need to play something from mary poppins at them , everyone knows that ` s how rooms get tidy\n",
            "#####Target######\n",
            "you need to play something from Mary Poppins at them, everyone knows that`s how rooms get tidy\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] 92 . 7 is turning it right about now . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "92 . 7 is turning it right about now .\n",
            "#####Target######\n",
            "92.7 is turning it right about now.\n",
            "#######END#######\n",
            "[CLS] positive [SEP] getting a little mowing the grass in this evening ! fun [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "fun\n",
            "#####Target######\n",
            "fun\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] glad it ` s friday . . . but bummed that i am officially working saturdays . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "glad it ` s friday . . . but bummed that i am officially working saturdays .\n",
            "#####Target######\n",
            "Glad it`s Friday... but bummed that I am officially working Saturdays.\n",
            "#######END#######\n",
            "[CLS] negative [SEP] woo ! twitter kind of sucked without you . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "kind of sucked\n",
            "#####Target######\n",
            "sucked\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] playing the game of watching straight people hook up but can ` t leave because of another reason [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "playing the game of watching straight people hook up but can ` t leave because of another reason\n",
            "#####Target######\n",
            "playing the game of watching straight people hook up but can`t leave because of another reason\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] my day was good ! spent the day catching up on sleep , relaxing . . kind of a lazy day ! hehe . oooo shopping ! how did you resist ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "my day was good ! spent the day catching up on sleep , relaxing . . kind of a lazy day ! hehe . oooo shopping ! how did you resist ?\n",
            "#####Target######\n",
            "My day was good!  Spent the day catching up on sleep, relaxing.. kind of a lazy day! hehe. oooo shopping\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] ya mine too but for very different reason [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "ya mine too but for very different reason\n",
            "#####Target######\n",
            "ya mine too but for very different reason\n",
            "#######END#######\n",
            "[CLS] positive [SEP] restoring new itouch 2 . excited to use it . yeeeeeeeee . goodbye itouch 1 . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "restoring . excited to use it . yeeeeeeeee .\n",
            "#####Target######\n",
            "Excited to use it.\n",
            "#######END#######\n",
            "[CLS] negative [SEP] i feel like * * * * . . . . . . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "i feel like * * * * . . .\n",
            "#####Target######\n",
            "I FEEL LIKE ****......\n",
            "#######END#######\n",
            "[CLS] positive [SEP] goodnight ; [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "goodnight ;\n",
            "#####Target######\n",
            "Goodnight;\n",
            "#######END#######\n",
            "[CLS] positive [SEP] oooh harlow is so sweet [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "sweet\n",
            "#####Target######\n",
            "so sweet\n",
            "#######END#######\n",
            "[CLS] negative [SEP] i miss oklahomaaaaa listening to citizen cope til i pass out . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "i miss oklahomaaaaa listening to citizen cope til i pass out .\n",
            "#####Target######\n",
            "miss\n",
            "#######END#######\n",
            "[CLS] negative [SEP] gross ! haha it was like the tiniest piece ever but i can taste it like i ate the whole onion eww [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "gross !\n",
            "#####Target######\n",
            "Gross!\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] check and check ! and yes , i did ( and am ) ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "check and check ! and yes , i did ( and am ) !\n",
            "#####Target######\n",
            "Check and check!   And yes, I did (and am)!\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] you stopped followin me ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "you stopped followin me ?\n",
            "#####Target######\n",
            "you stopped followin me?\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] yes , yes . . . . nap or read gossipy mag if you have one handy [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "yes , yes . . . . nap or read gossipy mag if you have one handy\n",
            "#####Target######\n",
            "yes, yes .... nap or read gossipy mag if you have one handy\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] it is very cheeky one , but nice too [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "it is very cheeky one , but nice too\n",
            "#####Target######\n",
            "it is very cheeky one, but nice too\n",
            "#######END#######\n",
            "[CLS] positive [SEP] _ com all breaking news will be tweeted here i hope welcome to twitter ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "i hope welcome to twitter !\n",
            "#####Target######\n",
            "hope\n",
            "#######END#######\n",
            "[CLS] neutral [SEP] just got back from the karaoke bar in the metreon ! ! ! watching millionaire matchmaker [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "######Our output##########\n",
            "just got back from the karaoke bar in the metreon ! ! ! watching millionaire matchmaker\n",
            "#####Target######\n",
            "Just got back from the karaoke bar in the metreon!!! Watching millionaire matchmaker\n",
            "#######END#######\n",
            "2055.106372646818\n",
            "0.6850354575489392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlegzxQx6y9q"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1o6I81q2z2o"
      },
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "test = test.dropna()\n",
        "\n",
        "\n",
        "test_textID = test.textID.tolist()\n",
        "test_text = test.text.tolist()\n",
        "test_sentiment = test.sentiment.tolist()\n",
        "\n",
        "input_idx = []\n",
        "input_mask = []\n",
        "output_idx = []\n",
        "for j in range( len(test_text) ):\n",
        "\n",
        "  CLS_id = text_to_ids(\"[CLS]\")\n",
        "  sentiment_id = text_to_ids(test_sentiment[j])\n",
        "  SEP_id = text_to_ids(\"[SEP]\")\n",
        "  test_text_ids = text_to_ids(test_text[j])\n",
        "  \n",
        "  tokens_ids = CLS_id + sentiment_id + SEP_id + test_text_ids \n",
        "  input_idx.append(padding(tokens_ids))\n",
        "\n",
        "  \n",
        "  mask = [1]* (len(test_text[j])+3)\n",
        "  input_mask.append(padding(mask))\n",
        "\n",
        "\n",
        "test_data = TensorDataset(torch.from_numpy(np.array(input_idx)),\\\n",
        "              torch.from_numpy(np.array(input_mask)),\\\n",
        "               )\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size= batch_size)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  outputs= []\n",
        "  score = 0.0\n",
        "  check = False \n",
        "  for idx, (orin_input_idx, attention_mask) in enumerate(test_loader):\n",
        "      input_idx=torch.tensor(orin_input_idx,dtype=torch.long)\n",
        "      attention_mask = torch.tensor(attention_mask,dtype=torch.long)\n",
        "      input_idx, attention_mask = cuda(input_idx),cuda(attention_mask)\n",
        "      out = model(input_idx, attention_mask)\n",
        "      answers = out_to_class(out)\n",
        "      \n",
        "      for i,answer in enumerate(answers):\n",
        "        orig_ids = orin_input_idx[i].numpy()\n",
        "        output = boolean_to_string(answer, orig_ids)\n",
        "        outputs.append(output)\n",
        "  print (outputs)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0MljTCk95lF"
      },
      "source": [
        "df= pd.DataFrame()\n",
        "test['test'] = outputs\n",
        "test.to_csv('model5_rlt.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}